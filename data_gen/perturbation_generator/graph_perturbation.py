"""
å›¾æ‰°åŠ¨æ¨¡å— - å¯¹ NetworkX MultiDiGraph è¿›è¡Œå„ç§ç±»å‹çš„æ‰°åŠ¨

æ”¯æŒçš„æ‰°åŠ¨ç±»å‹ï¼š
1. incomplete_edges: åˆ é™¤è¾¹æ¨¡æ‹Ÿæ•°æ®ä¸å®Œæ•´
2. false_edges: æ·»åŠ å‡è¾¹æ¨¡æ‹Ÿé”™è¯¯å…³ç³»
3. relation_type_noise: æ›¿æ¢å…³ç³»ç±»å‹æ¨¡æ‹Ÿæå–è¯¯åˆ†ç±»
4. node_type_noise: æ›¿æ¢èŠ‚ç‚¹ç±»å‹æ¨¡æ‹Ÿå®ä½“åˆ†ç±»é”™è¯¯
5. name_typos: æ³¨å…¥å­—ç¬¦çº§å™ªå£°æ¨¡æ‹ŸOCR/NLPé”™è¯¯
"""

import json
import random
import copy
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import networkx as nx
import numpy as np


class PerturbationType(Enum):
    """æ‰°åŠ¨ç±»å‹æšä¸¾"""
    INCOMPLETE_EDGES = "incomplete_edges"
    FALSE_EDGES = "false_edges"
    RELATION_TYPE_NOISE = "relation_type_noise"
    NODE_TYPE_NOISE = "node_type_noise"
    NAME_TYPOS = "name_typos"
    ATTRIBUTE_NOISE = "attribute_noise"

@dataclass
class EdgeRecord:
    """è¾¹æ‰°åŠ¨è®°å½•"""
    source: str
    target: str
    edge_key: int
    original_attrs: Dict[str, Any]
    new_attrs: Optional[Dict[str, Any]] = None
    operation: str = ""  # 'deleted', 'added', 'modified'
    
    def to_dict(self) -> Dict:
        return {
            "source": self.source,
            "target": self.target,
            "edge_key": self.edge_key,
            "original_attrs": self.original_attrs,
            "new_attrs": self.new_attrs,
            "operation": self.operation
        }


@dataclass
class NodeRecord:
    """èŠ‚ç‚¹æ‰°åŠ¨è®°å½•"""
    node_id: str
    original_attrs: Dict[str, Any]
    new_attrs: Optional[Dict[str, Any]] = None
    operation: str = ""  # 'modified'
    field_changed: str = ""  # æ”¹å˜çš„å­—æ®µå
    
    def to_dict(self) -> Dict:
        return {
            "node_id": self.node_id,
            "original_attrs": self.original_attrs,
            "new_attrs": self.new_attrs,
            "operation": self.operation,
            "field_changed": self.field_changed
        }


@dataclass
class PerturbationLog:
    """æ‰°åŠ¨æ—¥å¿—è®°å½•"""
    perturbation_type: str
    timestamp: str = ""
    records: List[Dict] = field(default_factory=list)
    summary: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        return {
            "perturbation_type": self.perturbation_type,
            "timestamp": self.timestamp,
            "records": self.records,
            "summary": self.summary
        }


class GraphPerturbation:
    """
    å›¾æ‰°åŠ¨ç±» - å¯¹ NetworkX MultiDiGraph è¿›è¡Œå„ç§ç±»å‹çš„æ‰°åŠ¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. åŠ è½½æ‰°åŠ¨é…ç½®æ–‡ä»¶
    2. æ‰§è¡Œå„ç§ç±»å‹çš„æ‰°åŠ¨
    3. è®°å½•æ‰€æœ‰æ‰°åŠ¨æ“ä½œçš„è¯¦ç»†ä¿¡æ¯
    4. ç»´æŠ¤å™ªå£°èŠ‚ç‚¹å’Œå™ªå£°è¾¹çš„åˆ—è¡¨
    """
    
    def __init__(self, graph: nx.MultiDiGraph, guide_file: Optional[str] = None):
        """
        åˆå§‹åŒ–å›¾æ‰°åŠ¨å™¨
        
        Args:
            graph: NetworkX MultiDiGraph å¯¹è±¡
            guide_file: æ‰°åŠ¨æŒ‡å¯¼æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
        """
        self.original_graph = graph
        self.graph = copy.deepcopy(graph)  # åˆ›å»ºå‰¯æœ¬è¿›è¡Œæ‰°åŠ¨
        
        # æ‰°åŠ¨é…ç½®
        self.guide_data: Dict = {}
        self.noise_profile: Dict[str, float] = {}
        self.noise_types: Dict = {}
        
        # æ‰°åŠ¨è®°å½•
        self.perturbation_logs: List[PerturbationLog] = []
        
        # å™ªå£°èŠ‚ç‚¹å’Œè¾¹çš„é›†åˆ
        self.noisy_nodes: Set[str] = set()  # è¢«æ‰°åŠ¨çš„èŠ‚ç‚¹IDé›†åˆ
        self.noisy_edges: Set[Tuple[str, str, int]] = set()  # è¢«æ‰°åŠ¨çš„è¾¹ (src, dst, key) é›†åˆ
        self.deleted_edges: List[EdgeRecord] = []  # è¢«åˆ é™¤çš„è¾¹è®°å½•
        self.added_edges: List[EdgeRecord] = []  # è¢«æ·»åŠ çš„è¾¹è®°å½•
        self.modified_edges: List[EdgeRecord] = []  # è¢«ä¿®æ”¹çš„è¾¹è®°å½•
        self.modified_nodes: List[NodeRecord] = []  # è¢«ä¿®æ”¹çš„èŠ‚ç‚¹è®°å½•
        
        # Embedding æ¨¡å‹ï¼ˆç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
        self._embedding_model = None
        self._embeddings_cache: Dict[str, np.ndarray] = {}
        
        # åŠ è½½æŒ‡å¯¼æ–‡ä»¶
        if guide_file:
            self._load_guide_file(guide_file)
    
    def _load_guide_file(self, guide_file_path: str) -> None:
        """åŠ è½½æ‰°åŠ¨æŒ‡å¯¼æ–‡ä»¶"""
        try:
            with open(guide_file_path, 'r', encoding='utf-8') as f:
                self.guide_data = json.load(f)
            
            self.noise_types = self.guide_data.get('noise_types', {})
            self.noise_profile = self.guide_data.get('default_profile', {})
            print(f"âœ… å·²åŠ è½½æ‰°åŠ¨æŒ‡å¯¼æ–‡ä»¶: {guide_file_path}")
            print(f"   - æ‰°åŠ¨ç±»å‹: {list(self.noise_types.keys())}")
            print(f"   - é»˜è®¤é…ç½®: {self.noise_profile}")
        except Exception as e:
            print(f"âŒ åŠ è½½æŒ‡å¯¼æ–‡ä»¶å¤±è´¥: {e}")
            self.guide_data = {}
    
    def set_noise_profile(self, profile: Dict[str, float]) -> None:
        """è®¾ç½®è‡ªå®šä¹‰æ‰°åŠ¨é…ç½®"""
        self.noise_profile = profile
        print(f"âœ… å·²è®¾ç½®è‡ªå®šä¹‰æ‰°åŠ¨é…ç½®: {profile}")
    
    # ==================== è¯­ä¹‰ç›¸ä¼¼åº¦ç›¸å…³æ–¹æ³• ====================
    
    def _get_embedding_model(self):
        """è·å–æˆ–åˆå§‹åŒ– embedding æ¨¡å‹ï¼ˆä½¿ç”¨ sentence-transformersï¼‰"""
        if self._embedding_model is None:
            try:
                from sentence_transformers import SentenceTransformer
                # ä½¿ç”¨è½»é‡çº§ä½†æ•ˆæœå¥½çš„æ¨¡å‹
                self._embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("âœ… å·²åŠ è½½ sentence-transformers æ¨¡å‹: all-MiniLM-L6-v2")
            except ImportError:
                print("âš ï¸ è­¦å‘Š: sentence-transformers æœªå®‰è£…ï¼Œå°†ä½¿ç”¨éšæœºé€‰æ‹©æ›¿ä»£è¯­ä¹‰ç›¸ä¼¼åº¦")
                print("   è¯·è¿è¡Œ: pip install sentence-transformers")
                return None
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ embedding æ¨¡å‹å¤±è´¥: {e}")
                return None
        return self._embedding_model
    
    def _get_embeddings(self, texts: List[str]) -> Optional[np.ndarray]:
        """
        è·å–æ–‡æœ¬çš„ embeddings
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            embeddings æ•°ç»„ï¼Œå¦‚æœæ¨¡å‹ä¸å¯ç”¨åˆ™è¿”å› None
        """
        model = self._get_embedding_model()
        if model is None:
            return None
        
        # ä½¿ç”¨ç¼“å­˜
        uncached_texts = [t for t in texts if t not in self._embeddings_cache]
        if uncached_texts:
            embeddings = model.encode(uncached_texts)
            for text, emb in zip(uncached_texts, embeddings):
                self._embeddings_cache[text] = emb
        
        return np.array([self._embeddings_cache[t] for t in texts])
    
    def _compute_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° [0, 1]
        """
        embeddings = self._get_embeddings([text1, text2])
        if embeddings is None:
            return 0.0
        
        emb1, emb2 = embeddings[0], embeddings[1]
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return float(similarity)
    
    def _find_most_similar(self, target: str, candidates: List[str], 
                          exclude_self: bool = True) -> Tuple[str, float]:
        """
        ä»å€™é€‰åˆ—è¡¨ä¸­æ‰¾åˆ°ä¸ç›®æ ‡æœ€ç›¸ä¼¼çš„æ–‡æœ¬
        
        Args:
            target: ç›®æ ‡æ–‡æœ¬
            candidates: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            exclude_self: æ˜¯å¦æ’é™¤ä¸è‡ªèº«ç›¸åŒçš„æ–‡æœ¬
            
        Returns:
            Tuple[str, float]: (æœ€ç›¸ä¼¼çš„æ–‡æœ¬, ç›¸ä¼¼åº¦åˆ†æ•°)
        """
        if not candidates:
            return target, 1.0
        
        # è¿‡æ»¤å€™é€‰
        filtered_candidates = [c for c in candidates if c != target] if exclude_self else candidates
        if not filtered_candidates:
            return target, 1.0
        
        # è·å–æ‰€æœ‰ embeddings
        all_texts = [target] + filtered_candidates
        embeddings = self._get_embeddings(all_texts)
        
        if embeddings is None:
            # å¦‚æœæ— æ³•è·å– embeddingsï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
            return random.choice(filtered_candidates), 0.5
        
        target_emb = embeddings[0]
        candidate_embs = embeddings[1:]
        
        # è®¡ç®—æ‰€æœ‰å€™é€‰çš„ç›¸ä¼¼åº¦
        similarities = []
        for i, cand_emb in enumerate(candidate_embs):
            sim = np.dot(target_emb, cand_emb) / (np.linalg.norm(target_emb) * np.linalg.norm(cand_emb))
            similarities.append((filtered_candidates[i], float(sim)))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›æœ€ç›¸ä¼¼çš„
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[0]
    
    def apply_all_perturbations(self) -> nx.MultiDiGraph:
        """
        åº”ç”¨æ‰€æœ‰é…ç½®çš„æ‰°åŠ¨
        
        Returns:
            æ‰°åŠ¨åçš„å›¾
        """
        import datetime
        
        print(f"\n{'='*60}")
        print("å¼€å§‹åº”ç”¨å›¾æ‰°åŠ¨...")
        print(f"{'='*60}")
        print(f"åŸå§‹å›¾ç»Ÿè®¡: èŠ‚ç‚¹æ•°={self.graph.number_of_nodes():,}, è¾¹æ•°={self.graph.number_of_edges():,}")
        
        for perturbation_type, ratio in self.noise_profile.items():
            if ratio <= 0:
                continue
            
            print(f"\nğŸ“Œ åº”ç”¨æ‰°åŠ¨: {perturbation_type} (æ¯”ä¾‹: {ratio})")
            
            log = PerturbationLog(
                perturbation_type=perturbation_type,
                timestamp=datetime.datetime.now().isoformat()
            )
            
            if perturbation_type == PerturbationType.INCOMPLETE_EDGES.value:
                records, summary = self._apply_incomplete_edges(ratio)
            elif perturbation_type == PerturbationType.FALSE_EDGES.value:
                records, summary = self._apply_false_edges(ratio)
            elif perturbation_type == PerturbationType.RELATION_TYPE_NOISE.value:
                records, summary = self._apply_relation_type_noise(ratio)
            elif perturbation_type == PerturbationType.NODE_TYPE_NOISE.value:
                records, summary = self._apply_node_type_noise(ratio)
            elif perturbation_type == PerturbationType.NAME_TYPOS.value:
                records, summary = self._apply_name_typos(ratio)
            elif perturbation_type == PerturbationType.ATTRIBUTE_NOISE.value:
                records, summary = self._apply_attribute_noise(ratio)
            else:
                print(f"   âš ï¸ æœªçŸ¥æ‰°åŠ¨ç±»å‹: {perturbation_type}")
                continue
            
            log.records = records
            log.summary = summary
            self.perturbation_logs.append(log)
            
            print(f"   âœ… å®Œæˆ: {summary}")
        
        print(f"\n{'='*60}")
        print("æ‰°åŠ¨å®Œæˆ!")
        print(f"{'='*60}")
        print(f"æ‰°åŠ¨åå›¾ç»Ÿè®¡: èŠ‚ç‚¹æ•°={self.graph.number_of_nodes():,}, è¾¹æ•°={self.graph.number_of_edges():,}")
        print(f"å™ªå£°èŠ‚ç‚¹æ•°: {len(self.noisy_nodes):,}")
        print(f"å™ªå£°è¾¹æ•°: {len(self.noisy_edges):,}")
        print(f"{'='*60}\n")
        
        return self.graph
    
    # ==================== æ‰°åŠ¨ç®—æ³•å®ç° ====================
    
    def _apply_incomplete_edges(self, ratio: float) -> Tuple[List[Dict], Dict]:
        """
        åº”ç”¨ä¸å®Œæ•´è¾¹æ‰°åŠ¨ - åˆ é™¤è¾¹æ¨¡æ‹Ÿæ•°æ®ä¸å®Œæ•´
        
        Args:
            ratio: è¦åˆ é™¤çš„è¾¹æ¯”ä¾‹
            
        Returns:
            (è®°å½•åˆ—è¡¨, æ‘˜è¦)
        """
        records = []
        edges = list(self.graph.edges(keys=True, data=True))
        num_to_delete = int(len(edges) * ratio)
        
        if num_to_delete == 0:
            return records, {"deleted_count": 0}
        
        # éšæœºé€‰æ‹©è¦åˆ é™¤çš„è¾¹
        edges_to_delete = random.sample(edges, min(num_to_delete, len(edges)))
        
        config = self.noise_types.get('incomplete_edges', {})
        avoid_dangling = config.get('constraints', {}).get('avoid_dangling_edges', True)
        
        deleted_count = 0
        for src, dst, key, data in edges_to_delete:
            # æ£€æŸ¥æ˜¯å¦ä¼šäº§ç”Ÿæ‚¬æŒ‚èŠ‚ç‚¹ï¼ˆåº¦ä¸º0çš„èŠ‚ç‚¹ï¼‰
            if avoid_dangling:
                src_degree = self.graph.degree(src)
                dst_degree = self.graph.degree(dst)
                if src_degree <= 1 or dst_degree <= 1:
                    continue  # è·³è¿‡ï¼Œé¿å…äº§ç”Ÿå­¤ç«‹èŠ‚ç‚¹
            
            # è®°å½•åŸå§‹è¾¹ä¿¡æ¯
            edge_record = EdgeRecord(
                source=src,
                target=dst,
                edge_key=key,
                original_attrs=dict(data),
                operation='deleted'
            )
            
            # åˆ é™¤è¾¹
            self.graph.remove_edge(src, dst, key=key)
            
            # æ›´æ–°è®°å½•
            self.deleted_edges.append(edge_record)
            self.noisy_edges.add((src, dst, key))
            records.append(edge_record.to_dict())
            deleted_count += 1
        
        summary = {
            "deleted_count": deleted_count,
            "target_count": num_to_delete
        }
        return records, summary
    
    def _apply_false_edges(self, ratio: float) -> Tuple[List[Dict], Dict]:
        """
        åº”ç”¨è™šå‡è¾¹æ‰°åŠ¨ - æ·»åŠ ä¸å­˜åœ¨çš„è¾¹
        
        Args:
            ratio: è¦æ·»åŠ çš„è¾¹æ¯”ä¾‹ï¼ˆç›¸å¯¹äºåŸå§‹è¾¹æ•°ï¼‰
            
        Returns:
            (è®°å½•åˆ—è¡¨, æ‘˜è¦)
        """
        records = []
        num_edges = self.graph.number_of_edges()
        num_to_add = int(num_edges * ratio)
        
        if num_to_add == 0:
            return records, {"added_count": 0}
        
        # è·å–æ‰€æœ‰èŠ‚ç‚¹å’Œç°æœ‰è¾¹
        nodes = list(self.graph.nodes(data=True))
        existing_edges = set((u, v) for u, v, _ in self.graph.edges(keys=True))
        
        # æ”¶é›†æ‰€æœ‰å…³ç³»ç±»å‹
        relation_types = set()
        for _, _, data in self.graph.edges(data=True):
            if 'label' in data:
                relation_types.add(data['label'])
            if 'relation' in data:
                relation_types.add(data['relation'])
        relation_types = list(relation_types) if relation_types else ['related_to']
        
        config = self.noise_types.get('false_edges', {})
        avoid_dangling = config.get('constraints', {}).get('avoid_dangling_edges', True)
        
        added_count = 0
        attempts = 0
        max_attempts = num_to_add * 10
        
        while added_count < num_to_add and attempts < max_attempts:
            attempts += 1
            
            # éšæœºé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„èŠ‚ç‚¹
            if len(nodes) < 2:
                break
            
            src_node, src_data = random.choice(nodes)
            dst_node, dst_data = random.choice(nodes)
            
            if src_node == dst_node:
                continue
            
            # æ£€æŸ¥è¾¹æ˜¯å¦å·²å­˜åœ¨
            if (src_node, dst_node) in existing_edges:
                continue
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªå…³ç³»ç±»å‹
            relation = random.choice(relation_types)
            
            # åˆ›å»ºè¾¹å±æ€§
            edge_attrs = {
                'label': relation,
                'relation': relation,
                'is_noise': True  # æ ‡è®°ä¸ºå™ªå£°è¾¹
            }
            
            # æ·»åŠ è¾¹
            edge_key = self.graph.add_edge(src_node, dst_node, **edge_attrs)
            
            # è®°å½•æ·»åŠ çš„è¾¹
            edge_record = EdgeRecord(
                source=src_node,
                target=dst_node,
                edge_key=edge_key if edge_key is not None else 0,
                original_attrs={},  # åŸå§‹ä¸å­˜åœ¨
                new_attrs=edge_attrs,
                operation='added'
            )
            
            self.added_edges.append(edge_record)
            self.noisy_edges.add((src_node, dst_node, edge_key if edge_key is not None else 0))
            existing_edges.add((src_node, dst_node))
            records.append(edge_record.to_dict())
            added_count += 1
        
        summary = {
            "added_count": added_count,
            "target_count": num_to_add
        }
        return records, summary
    
    def _apply_relation_type_noise(self, ratio: float) -> Tuple[List[Dict], Dict]:
        """
        åº”ç”¨å…³ç³»ç±»å‹å™ªå£° - ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ›¿æ¢è¾¹çš„å…³ç³»ç±»å‹
        
        æå–å…³ç³»ç±»å‹å­—æ®µçš„å€¼ï¼Œä½¿ç”¨ embedding æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ¯”è¾ƒï¼Œ
        é€‰å‡ºè¯­ä¹‰æœ€ç›¸è¿‘çš„å…³ç³»ç±»å‹è¿›è¡Œæ›¿æ¢ï¼Œæ¨¡æ‹Ÿæå–è¯¯åˆ†ç±»ã€‚
        
        Args:
            ratio: è¦ä¿®æ”¹çš„è¾¹æ¯”ä¾‹
            
        Returns:
            (è®°å½•åˆ—è¡¨, æ‘˜è¦)
        """
        records = []
        edges = list(self.graph.edges(keys=True, data=True))
        num_to_modify = int(len(edges) * ratio)
        
        if num_to_modify == 0:
            return records, {"modified_count": 0}
        
        # æ”¶é›†æ‰€æœ‰å…³ç³»ç±»å‹
        relation_types = []
        for _, _, _, data in edges:
            if 'label' in data:
                relation_types.append(str(data['label']))
            elif 'relation' in data:
                relation_types.append(str(data['relation']))
        
        unique_relations = list(set(relation_types))
        
        if len(unique_relations) < 2:
            return records, {"modified_count": 0, "reason": "å…³ç³»ç±»å‹ä¸è¶³"}
        
        # éšæœºé€‰æ‹©è¦ä¿®æ”¹çš„è¾¹
        edges_to_modify = random.sample(edges, min(num_to_modify, len(edges)))
        
        # è·å–é…ç½®ä¸­çš„çº¦æŸæ¡ä»¶
        config = self.noise_types.get('relation_type_noise', {})
        same_family = config.get('constraints', {}).get('same_relation_family', True)
        
        modified_count = 0
        total_similarity = 0.0
        
        for src, dst, key, data in edges_to_modify:
            original_relation = data.get('label') or data.get('relation')
            
            if original_relation is None or str(original_relation).strip() == '':
                continue
            
            original_relation_str = str(original_relation)
            
            # è·å–å€™é€‰å…³ç³»ç±»å‹ï¼ˆæ’é™¤åŸå€¼ï¼‰
            candidate_relations = [r for r in unique_relations if r != original_relation_str]
            
            if not candidate_relations:
                continue
            
            # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å…³ç³»ç±»å‹
            if same_family:
                # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦é€‰æ‹©æœ€ç›¸è¿‘çš„å…³ç³»ç±»å‹
                new_relation, similarity = self._find_most_similar(
                    original_relation_str, 
                    candidate_relations
                )
            else:
                # éšæœºé€‰æ‹©ï¼ˆä¸è€ƒè™‘è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
                new_relation = random.choice(candidate_relations)
                similarity = 0.0
            
            total_similarity += similarity
            
            # ä¿å­˜åŸå§‹å±æ€§
            original_attrs = dict(data)
            
            # æ›´æ–°è¾¹å±æ€§
            if 'label' in self.graph[src][dst][key]:
                self.graph[src][dst][key]['label'] = new_relation
            if 'relation' in self.graph[src][dst][key]:
                self.graph[src][dst][key]['relation'] = new_relation
            if 'display_relation' in self.graph[src][dst][key]:
                self.graph[src][dst][key]['display_relation'] = new_relation
            
            # æ ‡è®°ä¸ºå™ªå£°å¹¶è®°å½•ç›¸ä¼¼åº¦
            self.graph[src][dst][key]['is_noise'] = True
            self.graph[src][dst][key]['noise_similarity'] = similarity
            
            # åˆ›å»ºè®°å½•ï¼ˆåŒ…å«ç›¸ä¼¼åº¦ä¿¡æ¯ï¼‰
            record_dict = {
                "source": src,
                "target": dst,
                "edge_key": key,
                "original_attrs": original_attrs,
                "new_attrs": dict(self.graph[src][dst][key]),
                "operation": "modified",
                "change": {
                    "original_value": original_relation_str,
                    "new_value": new_relation,
                    "modification_method": "semantic_similar_swap",
                    "similarity_score": similarity
                }
            }
            
            # è®°å½•ä¿®æ”¹
            edge_record = EdgeRecord(
                source=src,
                target=dst,
                edge_key=key,
                original_attrs=original_attrs,
                new_attrs=dict(self.graph[src][dst][key]),
                operation='modified'
            )
            
            self.modified_edges.append(edge_record)
            self.noisy_edges.add((src, dst, key))
            records.append(record_dict)
            modified_count += 1
        
        avg_similarity = total_similarity / modified_count if modified_count > 0 else 0.0
        
        summary = {
            "modified_count": modified_count,
            "target_count": num_to_modify,
            "average_similarity": round(avg_similarity, 4),
            "unique_relations_count": len(unique_relations)
        }
        return records, summary
    
    def _apply_node_type_noise(self, ratio: float) -> Tuple[List[Dict], Dict]:
        """
        åº”ç”¨èŠ‚ç‚¹ç±»å‹å™ªå£° - ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ›¿æ¢èŠ‚ç‚¹çš„ç±»å‹æ ‡ç­¾
        
        æå–èŠ‚ç‚¹ç±»å‹å­—æ®µçš„å€¼ï¼Œä½¿ç”¨ embedding æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ¯”è¾ƒï¼Œ
        é€‰å‡ºè¯­ä¹‰æœ€ç›¸è¿‘çš„èŠ‚ç‚¹ç±»å‹è¿›è¡Œæ›¿æ¢ï¼Œæ¨¡æ‹Ÿå®ä½“åˆ†ç±»é”™è¯¯ã€‚
        
        Args:
            ratio: è¦ä¿®æ”¹çš„èŠ‚ç‚¹æ¯”ä¾‹
            
        Returns:
            (è®°å½•åˆ—è¡¨, æ‘˜è¦)
        """
        records = []
        nodes = list(self.graph.nodes(data=True))
        num_to_modify = int(len(nodes) * ratio)
        
        if num_to_modify == 0:
            return records, {"modified_count": 0}
        
        # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹ç±»å‹
        node_types = []
        for _, data in nodes:
            if 'label' in data:
                node_types.append(str(data['label']))
            elif 'node_type' in data:
                node_types.append(str(data['node_type']))
        
        unique_types = list(set(node_types))
        
        if len(unique_types) < 2:
            return records, {"modified_count": 0, "reason": "èŠ‚ç‚¹ç±»å‹ä¸è¶³"}
        
        # è·å–é…ç½®ä¸­çš„çº¦æŸæ¡ä»¶
        config = self.noise_types.get('node_type_noise', {})
        allow_invalid = config.get('constraints', {}).get('allow_invalid_combinations', True)
        
        # éšæœºé€‰æ‹©è¦ä¿®æ”¹çš„èŠ‚ç‚¹
        nodes_to_modify = random.sample(nodes, min(num_to_modify, len(nodes)))
        
        modified_count = 0
        total_similarity = 0.0
        
        for node_id, data in nodes_to_modify:
            original_type = data.get('label') or data.get('node_type')
            
            if original_type is None or str(original_type).strip() == '':
                continue
            
            original_type_str = str(original_type)
            
            # è·å–å€™é€‰èŠ‚ç‚¹ç±»å‹ï¼ˆæ’é™¤åŸå€¼ï¼‰
            candidate_types = [t for t in unique_types if t != original_type_str]
            
            if not candidate_types:
                continue
            
            # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ‰¾åˆ°æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹ç±»å‹
            new_type, similarity = self._find_most_similar(
                original_type_str, 
                candidate_types
            )
            
            total_similarity += similarity
            
            # ä¿å­˜åŸå§‹å±æ€§
            original_attrs = dict(data)
            
            # æ›´æ–°èŠ‚ç‚¹å±æ€§
            if 'label' in self.graph.nodes[node_id]:
                self.graph.nodes[node_id]['label'] = new_type
            if 'node_type' in self.graph.nodes[node_id]:
                self.graph.nodes[node_id]['node_type'] = new_type
            
            # è®°å½•ç›¸ä¼¼åº¦
            self.graph.nodes[node_id]['noise_similarity'] = similarity
            
            # åˆ›å»ºè®°å½•ï¼ˆåŒ…å«ç›¸ä¼¼åº¦ä¿¡æ¯ï¼‰
            record_dict = {
                "node_id": node_id,
                "original_attrs": original_attrs,
                "new_attrs": dict(self.graph.nodes[node_id]),
                "operation": "modified",
                "field_changed": "node_type",
                "change": {
                    "original_value": original_type_str,
                    "new_value": new_type,
                    "modification_method": "semantic_similar_swap",
                    "similarity_score": similarity
                }
            }
            
            # è®°å½•ä¿®æ”¹
            node_record = NodeRecord(
                node_id=node_id,
                original_attrs=original_attrs,
                new_attrs=dict(self.graph.nodes[node_id]),
                operation='modified',
                field_changed='node_type'
            )
            
            self.modified_nodes.append(node_record)
            self.noisy_nodes.add(node_id)
            records.append(record_dict)
            modified_count += 1
        
        avg_similarity = total_similarity / modified_count if modified_count > 0 else 0.0
        
        summary = {
            "modified_count": modified_count,
            "target_count": num_to_modify,
            "average_similarity": round(avg_similarity, 4),
            "unique_types_count": len(unique_types)
        }
        return records, summary
    
    def _find_node_name(self, data: Dict[str, Any], config: Dict = None) -> Tuple[Optional[str], Optional[str]]:
        """
        æŸ¥æ‰¾èŠ‚ç‚¹åç§°ï¼Œæ”¯æŒå¤šç§å¯èƒ½çš„å±æ€§å
        
        Args:
            data: èŠ‚ç‚¹å±æ€§å­—å…¸
            config: é…ç½®å­—å…¸ï¼Œå¯åŒ…å« name_fields åˆ—è¡¨æŒ‡å®šè¦æŸ¥æ‰¾çš„å­—æ®µå
            
        Returns:
            Tuple[Optional[str], Optional[str]]: (èŠ‚ç‚¹åç§°å€¼, å±æ€§å)
        """
        # ä»é…ç½®ä¸­è·å–å¯èƒ½çš„åç§°å­—æ®µåˆ—è¡¨
        if config:
            name_fields = config.get('name_fields', None)
            if name_fields:
                for field in name_fields:
                    if field in data and isinstance(data[field], str) and len(data[field]) >= 2:
                        return data[field], field
        
        # é»˜è®¤ä¼˜å…ˆçº§ï¼šname > å…¶ä»–åŒ…å«"name"çš„å±æ€§
        # æ³¨æ„ï¼šä¸åŒ…å« 'id'ï¼Œå› ä¸ºèŠ‚ç‚¹çš„idä¸åº”è¯¥è¢«ä¿®æ”¹
        # 1. é¦–å…ˆå°è¯• 'name'
        if 'name' in data and isinstance(data['name'], str) and len(data['name']) >= 2:
            return data['name'], 'name'
        
        # 2. æŸ¥æ‰¾æ‰€æœ‰åŒ…å« "name" æˆ– "Name" çš„å±æ€§ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        # æ’é™¤ 'id' å’Œä»¥ 'id' ç»“å°¾çš„å±æ€§ï¼Œé¿å…ä¿®æ”¹èŠ‚ç‚¹æ ‡è¯†ç¬¦
        for attr_name, attr_value in data.items():
            if (attr_name.lower() != 'id' and 
                not attr_name.lower().endswith('id') and
                'name' in attr_name.lower() and 
                isinstance(attr_value, str) and 
                len(attr_value) >= 2):
                return attr_value, attr_name
        
        return None, None
    
    def _apply_name_typos(self, ratio: float) -> Tuple[List[Dict], Dict]:
        """
        åº”ç”¨åç§°æ‹¼å†™é”™è¯¯å™ªå£° - å‘èŠ‚ç‚¹åç§°æ³¨å…¥å­—ç¬¦çº§å™ªå£°
        
        Args:
            ratio: è¦ä¿®æ”¹çš„èŠ‚ç‚¹æ¯”ä¾‹
            
        Returns:
            (è®°å½•åˆ—è¡¨, æ‘˜è¦)
        """
        records = []
        nodes = list(self.graph.nodes(data=True))
        num_to_modify = int(len(nodes) * ratio)
        
        if num_to_modify == 0:
            return records, {"modified_count": 0}
        
        config = self.noise_types.get('name_typos', {})
        typo_operations = config.get('operations', 
            ['character_substitution', 'character_deletion', 'character_insertion', 'case_alteration'])
        
        # éšæœºé€‰æ‹©è¦ä¿®æ”¹çš„èŠ‚ç‚¹
        nodes_to_modify = random.sample(nodes, min(num_to_modify, len(nodes)))
        
        modified_count = 0
        for node_id, data in nodes_to_modify:
            # å°è¯•è·å–èŠ‚ç‚¹åç§°ï¼ˆæ”¯æŒå¤šç§å±æ€§åï¼‰
            original_name, name_field = self._find_node_name(data, config)
            
            # å¦‚æœæ‰¾ä¸åˆ°åç§°å­—æ®µï¼Œè·³è¿‡
            if original_name is None or name_field is None:
                continue
            
            # ä¿æŠ¤ï¼šç¡®ä¿ä¸ä¼šä¿®æ”¹èŠ‚ç‚¹çš„idå­—æ®µ
            if name_field.lower() == 'id' or name_field.lower().endswith('id'):
                continue
            
            if len(original_name) < 3:
                continue
            
            # å¼•å…¥æ‹¼å†™é”™è¯¯
            noisy_name = self._introduce_typo(original_name, typo_operations)
            
            if noisy_name == original_name:
                continue
            
            # ä¿å­˜åŸå§‹å±æ€§
            original_attrs = dict(data)
            
            # æ›´æ–°èŠ‚ç‚¹åç§°ï¼ˆä½¿ç”¨æ‰¾åˆ°çš„å±æ€§åï¼‰
            self.graph.nodes[node_id][name_field] = noisy_name
            # å¦‚æœåŸå±æ€§åä¸æ˜¯ 'name'ï¼Œä¹Ÿä¿å­˜åŸå§‹å€¼åˆ° 'original_name' ä»¥ä¾¿è¿½è¸ª
            if name_field != 'name':
                self.graph.nodes[node_id]['original_name'] = original_name
            
            
            # è®°å½•ä¿®æ”¹
            node_record = NodeRecord(
                node_id=node_id,
                original_attrs=original_attrs,
                new_attrs=dict(self.graph.nodes[node_id]),
                operation='modified',
                field_changed=name_field
            )
            
            self.modified_nodes.append(node_record)
            self.noisy_nodes.add(node_id)
            records.append(node_record.to_dict())
            modified_count += 1
        
        summary = {
            "modified_count": modified_count,
            "target_count": num_to_modify
        }
        return records, summary
    
    def _apply_attribute_noise(self, ratio: float) -> Tuple[List[Dict], Dict]:
        """
        åº”ç”¨å±æ€§å™ªå£° - å¯¹èŠ‚ç‚¹å±æ€§è¿›è¡Œæ‰°åŠ¨
        
        å¯¹äºæ•°å€¼å±æ€§ï¼šå°†æ•°å€¼ä¹˜ä»¥éšæœºå€æ•°å› å­ [10, 100, 1000, 10000] æˆ– [1/10, 1/100, 1/1000, 1/10000]
        å¯¹äºå­—ç¬¦ä¸²å±æ€§ï¼šä½¿ç”¨æ‹¼å†™é”™è¯¯æ³¨å…¥ï¼ˆç±»ä¼¼ _introduce_typoï¼‰
        
        Args:
            ratio: è¦ä¿®æ”¹çš„èŠ‚ç‚¹æ¯”ä¾‹
            
        Returns:
            (è®°å½•åˆ—è¡¨, æ‘˜è¦)
        """
        records = []
        nodes = list(self.graph.nodes(data=True))
        num_to_modify = int(len(nodes) * ratio)
        
        if num_to_modify == 0:
            return records, {"modified_count": 0}
        
        config = self.noise_types.get('attribute_noise', {})
        # è·å–è¦æ’é™¤çš„å±æ€§ï¼ˆå¦‚èŠ‚ç‚¹IDã€ç±»å‹ç­‰ä¸åº”è¢«æ‰°åŠ¨çš„å±æ€§ï¼‰
        exclude_attrs = config.get('exclude_attributes', ['id', 'label', 'node_type', 'name'])
        # è·å–å­—ç¬¦ä¸²å±æ€§çš„æ‹¼å†™é”™è¯¯æ“ä½œç±»å‹
        typo_operations = config.get('typo_operations', 
            ['character_substitution', 'character_deletion', 'character_insertion', 'case_alteration'])
        
        # éšæœºé€‰æ‹©è¦ä¿®æ”¹çš„èŠ‚ç‚¹
        nodes_to_modify = random.sample(nodes, min(num_to_modify, len(nodes)))
        
        modified_count = 0
        numeric_attrs_count = 0
        string_attrs_count = 0
        
        for node_id, data in nodes_to_modify:
            # ä¿å­˜åŸå§‹å±æ€§
            original_attrs = dict(data)
            new_attrs = dict(data)
            modified = False
            changed_fields = []
            
            # éå†æ‰€æœ‰å±æ€§
            for attr_name, attr_value in data.items():
                # è·³è¿‡æ’é™¤çš„å±æ€§
                if attr_name in exclude_attrs:
                    continue
                
                # ä¿æŠ¤ï¼šç¡®ä¿ä¸ä¼šä¿®æ”¹èŠ‚ç‚¹çš„idå­—æ®µï¼ˆåŒ…æ‹¬æ‰€æœ‰ä»¥'id'ç»“å°¾çš„å±æ€§ï¼‰
                if attr_name.lower() == 'id' or attr_name.lower().endswith('id'):
                    continue
                
                # åˆ¤æ–­å±æ€§ç±»å‹å¹¶åº”ç”¨ç›¸åº”çš„æ‰°åŠ¨
                if isinstance(attr_value, (int, float)):
                    # æ•°å€¼å±æ€§ï¼šä¹˜ä»¥éšæœºå€æ•°å› å­
                    # å€æ•°å› å­é€‰é¡¹ï¼šæ”¾å¤§ [10, 100, 1000, 10000] æˆ–ç¼©å° [1/10, 1/100, 1/1000, 1/10000]
                    multipliers = [10, 100, 1000, 10000, 0.1, 0.01, 0.001, 0.0001]
                    multiplier = random.choice(multipliers)
                    new_value = attr_value * multiplier
                    
                    # å¦‚æœåŸå€¼æ˜¯æ•´æ•°ä¸”ç»“æœä¹Ÿæ˜¯æ•´æ•°ï¼Œä¿æŒä¸ºæ•´æ•°ï¼›å¦åˆ™è½¬ä¸ºæµ®ç‚¹æ•°
                    if isinstance(attr_value, int) and isinstance(new_value, float):
                        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•´æ•°ï¼ˆè€ƒè™‘æµ®ç‚¹è¯¯å·®ï¼‰
                        if abs(new_value - round(new_value)) < 1e-10:
                            new_value = int(round(new_value))
                    
                    new_attrs[attr_name] = new_value
                    modified = True
                    changed_fields.append(attr_name)
                    numeric_attrs_count += 1
                    
                elif isinstance(attr_value, str) and len(attr_value) >= 2:
                    # å­—ç¬¦ä¸²å±æ€§ï¼šä½¿ç”¨æ‹¼å†™é”™è¯¯æ³¨å…¥
                    noisy_value = self._introduce_typo(attr_value, typo_operations)
                    if noisy_value != attr_value:
                        new_attrs[attr_name] = noisy_value
                        modified = True
                        changed_fields.append(attr_name)
                        string_attrs_count += 1
            
            # å¦‚æœæœ‰ä¿®æ”¹ï¼Œæ›´æ–°èŠ‚ç‚¹å¹¶è®°å½•
            if modified:
                # æ›´æ–°å›¾ä¸­çš„èŠ‚ç‚¹å±æ€§ï¼ˆåªæ›´æ–°è¢«ä¿®æ”¹çš„å±æ€§ï¼‰
                for attr_name in changed_fields:
                    self.graph.nodes[node_id][attr_name] = new_attrs[attr_name]
                
                
                # åˆ›å»ºè®°å½•
                record_dict = {
                    "node_id": node_id,
                    "original_attrs": original_attrs,
                    "new_attrs": new_attrs,
                    "operation": "modified",
                    "field_changed": ",".join(changed_fields),
                    "change": {
                        "changed_fields": changed_fields,
                        "numeric_attrs_count": sum(1 for f in changed_fields 
                                                  if isinstance(original_attrs.get(f), (int, float))),
                        "string_attrs_count": sum(1 for f in changed_fields 
                                                 if isinstance(original_attrs.get(f), str))
                    }
                }
                
                # è®°å½•ä¿®æ”¹
                node_record = NodeRecord(
                    node_id=node_id,
                    original_attrs=original_attrs,
                    new_attrs=new_attrs,
                    operation='modified',
                    field_changed=",".join(changed_fields)
                )
                
                self.modified_nodes.append(node_record)
                self.noisy_nodes.add(node_id)
                records.append(record_dict)
                modified_count += 1
        
        summary = {
            "modified_count": modified_count,
            "target_count": num_to_modify,
            "numeric_attrs_modified": numeric_attrs_count,
            "string_attrs_modified": string_attrs_count
        }
        return records, summary
    
    def _introduce_typo(self, text: str, operations: List[str]) -> str:
        """
        å‘æ–‡æœ¬ä¸­å¼•å…¥æ‹¼å†™é”™è¯¯
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            operations: å¯ç”¨çš„æ‹¼å†™é”™è¯¯æ“ä½œåˆ—è¡¨
            
        Returns:
            å¸¦æœ‰æ‹¼å†™é”™è¯¯çš„æ–‡æœ¬
        """
        if not text or len(text) < 2:
            return text
        
        operation = random.choice(operations)
        text_list = list(text)
        
        if operation == 'character_substitution' and len(text_list) > 0:
            # å­—ç¬¦æ›¿æ¢ - ä½¿ç”¨ç›¸ä¼¼å­—ç¬¦
            pos = random.randint(0, len(text_list) - 1)
            similar_chars = {
                'a': ['o', 'e', 'q'], 'b': ['d', 'p'], 'c': ['e', 'o'],
                'd': ['b', 'p'], 'e': ['a', 'i', 'o'], 'f': ['t'],
                'g': ['q', 'j'], 'h': ['n', 'b'], 'i': ['l', '1', 'j'],
                'j': ['i', 'g'], 'k': ['x'], 'l': ['i', '1', 't'],
                'm': ['n', 'w'], 'n': ['m', 'h'], 'o': ['0', 'a', 'e'],
                'p': ['b', 'd', 'q'], 'q': ['g', 'p'], 'r': ['t'],
                's': ['5', 'z'], 't': ['f', 'l', '7'], 'u': ['v', 'w'],
                'v': ['u', 'w'], 'w': ['v', 'm'], 'x': ['k', 'z'],
                'y': ['v', 'j'], 'z': ['s', 'x'],
                '0': ['o', 'O'], '1': ['l', 'i', 'I'], '5': ['s', 'S'],
                '7': ['t', 'T']
            }
            char = text_list[pos].lower()
            if char in similar_chars:
                replacement = random.choice(similar_chars[char])
                # ä¿æŒåŸå§‹å¤§å°å†™
                if text_list[pos].isupper():
                    replacement = replacement.upper()
                text_list[pos] = replacement
            else:
                # éšæœºæ›¿æ¢
                if text_list[pos].isalpha():
                    alphabet = 'abcdefghijklmnopqrstuvwxyz'
                    if text_list[pos].isupper():
                        alphabet = alphabet.upper()
                    text_list[pos] = random.choice(alphabet)
        
        elif operation == 'character_deletion' and len(text_list) > 2:
            # å­—ç¬¦åˆ é™¤
            pos = random.randint(1, len(text_list) - 2)  # é¿å…åˆ é™¤é¦–å°¾å­—ç¬¦
            text_list.pop(pos)
        
        elif operation == 'character_insertion':
            # å­—ç¬¦æ’å…¥
            pos = random.randint(1, len(text_list) - 1)  # åœ¨ä¸­é—´æ’å…¥
            # æ’å…¥ç›¸é‚»ä½ç½®çš„å­—ç¬¦å‰¯æœ¬ï¼ˆå¸¸è§æ‰“å­—é”™è¯¯ï¼‰
            char_to_insert = text_list[pos - 1] if random.random() > 0.5 else text_list[pos]
            text_list.insert(pos, char_to_insert)
        
        elif operation == 'case_alteration' and len(text_list) > 0:
            # å¤§å°å†™æ”¹å˜
            pos = random.randint(0, len(text_list) - 1)
            if text_list[pos].isalpha():
                text_list[pos] = text_list[pos].swapcase()
        
        elif operation == 'mix':
            # æ··åˆå¤šç§é”™è¯¯
            available_ops = ['character_substitution', 'character_deletion', 
                           'character_insertion', 'case_alteration']
            selected_ops = random.sample(available_ops, min(2, len(available_ops)))
            for op in selected_ops:
                text_list = list(self._introduce_typo(''.join(text_list), [op]))
        
        return ''.join(text_list)
    
    # ==================== ç»“æœè·å–å’Œä¿å­˜ ====================
    
    def get_perturbed_graph(self) -> nx.MultiDiGraph:
        """è·å–æ‰°åŠ¨åçš„å›¾"""
        return self.graph
    
    def get_noisy_nodes(self) -> Set[str]:
        """è·å–æ‰€æœ‰å™ªå£°èŠ‚ç‚¹çš„IDé›†åˆ"""
        return self.noisy_nodes
    
    def get_noisy_edges(self) -> Set[Tuple[str, str, int]]:
        """è·å–æ‰€æœ‰å™ªå£°è¾¹çš„é›†åˆ (source, target, key)"""
        return self.noisy_edges
    
    def get_perturbation_summary(self) -> Dict:
        """è·å–æ‰°åŠ¨æ‘˜è¦"""
        return {
            "original_graph": {
                "nodes": self.original_graph.number_of_nodes(),
                "edges": self.original_graph.number_of_edges()
            },
            "perturbed_graph": {
                "nodes": self.graph.number_of_nodes(),
                "edges": self.graph.number_of_edges()
            },
            "noise_statistics": {
                "noisy_nodes_count": len(self.noisy_nodes),
                "noisy_edges_count": len(self.noisy_edges),
                "deleted_edges_count": len(self.deleted_edges),
                "added_edges_count": len(self.added_edges),
                "modified_edges_count": len(self.modified_edges),
                "modified_nodes_count": len(self.modified_nodes)
            },
            "perturbation_logs": [log.to_dict() for log in self.perturbation_logs]
        }
    
    def get_detailed_records(self) -> Dict:
        """è·å–è¯¦ç»†çš„æ‰°åŠ¨è®°å½•"""
        return {
            "deleted_edges": [r.to_dict() for r in self.deleted_edges],
            "added_edges": [r.to_dict() for r in self.added_edges],
            "modified_edges": [r.to_dict() for r in self.modified_edges],
            "modified_nodes": [r.to_dict() for r in self.modified_nodes],
            "noisy_nodes_list": list(self.noisy_nodes),
            "noisy_edges_list": [(s, t, k) for s, t, k in self.noisy_edges]
        }
    
    def save_perturbed_graph(self, output_path: str) -> None:
        """
        ä¿å­˜æ‰°åŠ¨åçš„å›¾
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆ.gpickleæ ¼å¼ï¼‰
        """
        with open(output_path, 'wb') as f:
            pickle.dump(self.graph, f, pickle.HIGHEST_PROTOCOL)
        print(f"âœ… æ‰°åŠ¨åçš„å›¾å·²ä¿å­˜: {output_path}")
    
    def save_perturbation_records(self, output_path: str) -> None:
        """
        ä¿å­˜æ‰°åŠ¨è®°å½•åˆ° JSON æ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆ.jsonæ ¼å¼ï¼‰
        """
        records = {
            "summary": self.get_perturbation_summary(),
            "detailed_records": self.get_detailed_records()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ‰°åŠ¨è®°å½•å·²ä¿å­˜: {output_path}")
    
    def save_noisy_elements(self, nodes_path: str, edges_path: str) -> None:
        """
        åˆ†åˆ«ä¿å­˜å™ªå£°èŠ‚ç‚¹å’Œå™ªå£°è¾¹åˆ—è¡¨
        
        Args:
            nodes_path: å™ªå£°èŠ‚ç‚¹åˆ—è¡¨è¾“å‡ºè·¯å¾„
            edges_path: å™ªå£°è¾¹åˆ—è¡¨è¾“å‡ºè·¯å¾„
        """
        # ä¿å­˜å™ªå£°èŠ‚ç‚¹
        with open(nodes_path, 'w', encoding='utf-8') as f:
            json.dump({
                "noisy_nodes": list(self.noisy_nodes),
                "count": len(self.noisy_nodes)
            }, f, ensure_ascii=False, indent=2)
        print(f"âœ… å™ªå£°èŠ‚ç‚¹åˆ—è¡¨å·²ä¿å­˜: {nodes_path}")
        
        # ä¿å­˜å™ªå£°è¾¹
        with open(edges_path, 'w', encoding='utf-8') as f:
            json.dump({
                "noisy_edges": [(s, t, k) for s, t, k in self.noisy_edges],
                "count": len(self.noisy_edges)
            }, f, ensure_ascii=False, indent=2)
        print(f"âœ… å™ªå£°è¾¹åˆ—è¡¨å·²ä¿å­˜: {edges_path}")
    
    def save_all_with_timestamp(self, dataset_name: str, output_dir: str, 
                                 records_dir: Optional[str] = None,
                                 save_records: bool = True) -> Dict[str, str]:
        """
        ä¿å­˜æ‰°åŠ¨åçš„å›¾å’Œæ‰€æœ‰è®°å½•ï¼Œä½¿ç”¨æ—¶é—´æˆ³å‘½å
        
        å‘½åæ ¼å¼: {dataset_name}_noise_{timestamp}.gpickle
        
        Args:
            dataset_name: æ•°æ®é›†åç§°ï¼ˆå¦‚ "Primekg"ï¼‰
            output_dir: å›¾æ–‡ä»¶è¾“å‡ºç›®å½•è·¯å¾„
            records_dir: è®°å½•æ–‡ä»¶è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneåˆ™ä¸output_dirç›¸åŒï¼‰
            save_records: æ˜¯å¦åŒæ—¶ä¿å­˜æ‰°åŠ¨è®°å½•
            
        Returns:
            Dict[str, str]: åŒ…å«æ‰€æœ‰ä¿å­˜æ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        import datetime
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç¡®ä¿å›¾è¾“å‡ºç›®å½•å­˜åœ¨
        graph_output_path = Path(output_dir)
        graph_output_path.mkdir(parents=True, exist_ok=True)
        
        # ç¡®ä¿è®°å½•è¾“å‡ºç›®å½•å­˜åœ¨
        if records_dir:
            records_output_path = Path(records_dir)
        else:
            records_output_path = graph_output_path
        records_output_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        base_name = f"{dataset_name}_noise_{timestamp}"
        
        # æ–‡ä»¶è·¯å¾„
        graph_path = graph_output_path / f"{base_name}.gpickle"
        records_path = records_output_path / f"{base_name}_records.json"
        noisy_nodes_path = records_output_path / f"{base_name}_noisy_nodes.json"
        noisy_edges_path = records_output_path / f"{base_name}_noisy_edges.json"
        
        saved_files = {}
        
        # ä¿å­˜æ‰°åŠ¨åçš„å›¾
        self.save_perturbed_graph(str(graph_path))
        saved_files["graph"] = str(graph_path)
        
        # ä¿å­˜è®°å½•
        if save_records:
            self.save_perturbation_records(str(records_path))
            self.save_noisy_elements(str(noisy_nodes_path), str(noisy_edges_path))
            saved_files["records"] = str(records_path)
            saved_files["noisy_nodes"] = str(noisy_nodes_path)
            saved_files["noisy_edges"] = str(noisy_edges_path)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ æ–‡ä»¶ä¿å­˜å®Œæˆ")
        print(f"   å›¾æ–‡ä»¶ç›®å½•: {output_dir}")
        if records_dir:
            print(f"   è®°å½•æ–‡ä»¶ç›®å½•: {records_dir}")
        print(f"   åŸºç¡€åç§°: {base_name}")
        print(f"{'='*60}")
        
        return saved_files


def load_graph_from_gpickle(path: str) -> nx.MultiDiGraph:
    """ä» gpickle æ–‡ä»¶åŠ è½½å›¾"""
    with open(path, 'rb') as f:
        graph = pickle.load(f)
    print(f"âœ… å·²åŠ è½½å›¾: {path}")
    print(f"   - èŠ‚ç‚¹æ•°: {graph.number_of_nodes():,}")
    print(f"   - è¾¹æ•°: {graph.number_of_edges():,}")
    return graph

