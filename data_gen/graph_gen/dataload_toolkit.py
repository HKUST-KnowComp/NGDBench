""" 
temporarily done for only one data format, need to be extended to other data formats
"""
import os
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„ï¼Œä»¥ä¾¿æ­£ç¡®å¯¼å…¥ pipeline æ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import gzip
import pandas as pd
import networkx as nx
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Tuple, List
from data_analyser.graph_handler import GraphInspector
from pathlib import Path
import pickle
import random
# from torch_geometric.utils import to_networkx

def read_csv_gz(file_path: str) -> pd.DataFrame:
    # read the single .csv.gz file
    # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦ï¼Œå…ˆå°è¯• | åˆ†éš”ç¬¦ï¼ˆLDBC SNB BI æ ¼å¼å¸¸ç”¨ï¼‰
    # æ³¨æ„ï¼šLDBC SNB BI æ ¼å¼é€šå¸¸æ²¡æœ‰ headerï¼Œä½¿ç”¨ header=None
    try:
        # å…ˆå°è¯• | åˆ†éš”ç¬¦ï¼Œæ²¡æœ‰ header
        with gzip.open(file_path, 'rt', encoding='utf-8') as f:
            df = pd.read_csv(f, sep='|', header=None)
        # å¦‚æœåªæœ‰ä¸€åˆ—ï¼Œè¯´æ˜åˆ†éš”ç¬¦ä¸å¯¹ï¼Œå°è¯•é€—å·
        if len(df.columns) == 1:
            with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                df = pd.read_csv(f, sep=',', header=None)
    except Exception:
        # å¦‚æœ | åˆ†éš”ç¬¦å¤±è´¥ï¼Œå°è¯•é€—å·
        try:
            with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                df = pd.read_csv(f, sep=',', header=None)
        except Exception:
            # æœ€åå°è¯•è‡ªåŠ¨æ£€æµ‹ï¼ˆå¯èƒ½æœ‰ headerï¼‰
            with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                df = pd.read_csv(f)
    return df

def read_csv(file_path: str, sep: str = ',', header=0) -> pd.DataFrame:
    # read the single .csv file
    # header=0 è¡¨ç¤ºç¬¬ä¸€è¡Œæ˜¯åˆ—åï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
    # å¦‚æœ header=Noneï¼Œpandasä¸ä¼šå°†ç¬¬ä¸€è¡Œä½œä¸ºåˆ—åï¼Œè€Œæ˜¯ä½¿ç”¨æ•°å­—åˆ—å
    return pd.read_csv(file_path, sep=sep, encoding='utf-8', header=header)

def process_single_file_ldbcbi(file_path: str, folder_name: str, file_format: str) -> Tuple[List[Tuple[str, str, dict]], List[Tuple[str, str, str]]]:
    # Process the single file and return the nodes and edges (åŒ…å«æ‰€æœ‰å±æ€§)
    import time
    start_time = time.time()
    file_name = os.path.basename(file_path)
    # print(f"ğŸ“– å¼€å§‹å¤„ç†: {file_name}")
    
    nodes = []
    edges = []
    
    try:
        if file_format == ".csv.gz":
            df = read_csv_gz(file_path)
        # elif file_format == ".json":
        #     df = read_json(file_path)
        # elif file_format == ".jsonl":
        #     df = read_jsonl(file_path)
        # elif file_format == ".parquet":
        #     df = read_parquet(file_path)
        # elif file_format == ".feather":
        #     df = read_feather(file_path)
        else:
            print(f"è­¦å‘Š: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ {file_format}ï¼Œè·³è¿‡æ–‡ä»¶ {file_path}")
            return nodes, edges
    except Exception as e:
        print(f"skip the file {file_path}, error: {e}")
        return nodes, edges
    
    # æ£€æŸ¥ DataFrame æ˜¯å¦ä¸ºç©º
    if df is None or df.empty:
        print(f"è­¦å‘Š: æ–‡ä»¶ {file_path} ä¸ºç©ºï¼Œè·³è¿‡")
        return nodes, edges
    
    # check if the file is a node table or an edge table
    if "_" not in folder_name:
        # the file is a node table
        node_type = folder_name
        # æ£€æŸ¥æ˜¯å¦æœ‰ 'id' åˆ—ï¼Œæˆ–è€…ç¬¬ä¸€åˆ—ï¼ˆå½“æ²¡æœ‰headeræ—¶ï¼Œåˆ—åæ˜¯0ï¼‰
        id_col = None
        if 'id' in df.columns:
            id_col = 'id'
        elif len(df.columns) > 0:
            # å¦‚æœæ²¡æœ‰ 'id' åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸º id
            id_col = df.columns[0]
        
        if id_col is not None:
            # éå†æ¯ä¸€è¡Œï¼Œæ„å»ºèŠ‚ç‚¹åŠå…¶æ‰€æœ‰å±æ€§
            for idx, row in df.iterrows():
                node_id_value = str(row[id_col])
                node_id = f"{node_type}:{node_id_value}"
                
                # æ„å»ºå±æ€§å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰åˆ—çš„å€¼
                attributes = {}
                for col in df.columns:
                    value = row[col]
                    # å¤„ç† NaN å€¼
                    if pd.isna(value):
                        attributes[col] = None
                    else:
                        # ä¿æŒåŸå§‹ç±»å‹ï¼Œä½†ç¡®ä¿å¯ä»¥åºåˆ—åŒ–
                        attributes[col] = value
                
                nodes.append((node_id, node_type, attributes))
    else:
        # the file is an edge table
        rel_type = folder_name
        cols = df.columns.tolist()
        if len(cols) >= 2:
            # å¤„ç†ä¸åŒåˆ—æ•°çš„è¾¹è¡¨ï¼š
            # - 2åˆ—ï¼šé™æ€è¾¹è¡¨ï¼Œç›´æ¥æ˜¯æºèŠ‚ç‚¹IDå’Œç›®æ ‡èŠ‚ç‚¹IDï¼ˆåˆ—0å’Œåˆ—1ï¼‰
            # - 3åˆ—æˆ–æ›´å¤šï¼šåŠ¨æ€è¾¹è¡¨ï¼Œç¬¬ä¸€åˆ—æ˜¯æ—¶é—´æˆ³ï¼Œç¬¬äºŒåˆ—å’Œç¬¬ä¸‰åˆ—æ˜¯æºèŠ‚ç‚¹IDå’Œç›®æ ‡èŠ‚ç‚¹IDï¼ˆåˆ—1å’Œåˆ—2ï¼‰
            if len(cols) == 2:
                # é™æ€è¾¹è¡¨ï¼šä½¿ç”¨ç¬¬0åˆ—å’Œç¬¬1åˆ—
                src_col, dst_col = cols[0], cols[1]
            else:
                # åŠ¨æ€è¾¹è¡¨ï¼šè·³è¿‡ç¬¬ä¸€åˆ—ï¼ˆæ—¶é—´æˆ³ï¼‰ï¼Œä½¿ç”¨ç¬¬1åˆ—å’Œç¬¬2åˆ—
                src_col, dst_col = cols[1], cols[2]
            
            # ä»å…³ç³»åï¼ˆæ–‡ä»¶å¤¹åï¼‰ä¸­æå–æºèŠ‚ç‚¹ç±»å‹å’Œç›®æ ‡èŠ‚ç‚¹ç±»å‹
            # æ ¼å¼ï¼šSourceType_RelationName_TargetType
            # ä¾‹å¦‚ï¼šPlace_isPartOf_Place -> æºç±»å‹ï¼šPlaceï¼Œç›®æ ‡ç±»å‹ï¼šPlace
            #      Tag_hasType_TagClass -> æºç±»å‹ï¼šTagï¼Œç›®æ ‡ç±»å‹ï¼šTagClass
            #      Comment_isLocatedIn_Country -> æºç±»å‹ï¼šCommentï¼Œç›®æ ‡ç±»å‹ï¼šCountry
            parts = folder_name.split('_')
            if len(parts) >= 3:
                # æºèŠ‚ç‚¹ç±»å‹æ˜¯ç¬¬ä¸€éƒ¨åˆ†
                src_prefix = parts[0]
                # ç›®æ ‡èŠ‚ç‚¹ç±»å‹æ˜¯æœ€åä¸€éƒ¨åˆ†
                dst_prefix = parts[-1]
                
                # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼šCountryã€Cityã€Universityã€Company ç­‰å¯èƒ½æ˜¯ Place æˆ– Organisation çš„å­ç±»å‹
                # åœ¨ composite-projected-fk æ ¼å¼ä¸­ï¼Œè¿™äº›èŠ‚ç‚¹å®é™…ä¸Šå­˜å‚¨åœ¨ Place æˆ– Organisation èŠ‚ç‚¹è¡¨ä¸­
                # å°†ç›®æ ‡èŠ‚ç‚¹ç±»å‹æ˜ å°„åˆ°å®é™…çš„èŠ‚ç‚¹ç±»å‹
                type_mapping = {
                    'Country': 'Place',
                    'City': 'Place',
                    'University': 'Organisation',  # æˆ–è€…å¯èƒ½æ˜¯ Placeï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    'Company': 'Organisation',
                }
                if dst_prefix in type_mapping:
                    dst_prefix = type_mapping[dst_prefix]
            else:
                # å¦‚æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œå°è¯•ä»åˆ—åä¸­æå–
                if isinstance(src_col, (int, str)) and str(src_col).isdigit():
                    src_prefix = "node"  # é»˜è®¤å‰ç¼€
                else:
                    src_prefix = str(src_col).split('_')[0] if '_' in str(src_col) else "node"
                
                if isinstance(dst_col, (int, str)) and str(dst_col).isdigit():
                    dst_prefix = "node"  # é»˜è®¤å‰ç¼€
                else:
                    dst_prefix = str(dst_col).split('_')[0] if '_' in str(dst_col) else "node"
            
            edges = []

            src_values = df[src_col].astype(str).values
            dst_values = df[dst_col].astype(str).values
            edges = [
                (f"{src_prefix}:{src}", f"{dst_prefix}:{dst}", rel_type)
                for src, dst in zip(src_values, dst_values)
            ]
    
    return nodes, edges

def build_graph_from_data_ldbcbi(data_path: str, file_format: str) -> nx.MultiDiGraph:
    """
    Build the graph from the data path (sequential processing)
    Suitable for single file or small dataset scenarios
    
    Args:
        data_path: Path to the data directory
        file_format: File format to process (e.g., ".csv.gz", ".csv")
        
    Returns:
        nx.MultiDiGraph: The constructed graph
    """
    import time
    overall_start = time.time()
    
    graph = nx.MultiDiGraph()
    print(f"loading graph data from {data_path}...")
    
    # collect all the files to be processed
    file_tasks = []
    for root, dirs, files in os.walk(data_path):
        for file in files:
            if not file.endswith(file_format):
                continue
            file_path = os.path.join(root, file)
            folder_name = os.path.basename(root)
            file_tasks.append((file_path, folder_name, file_format))
    
    total_files = len(file_tasks)
    print(f"found {total_files} files, start to process...")
    
    # process files sequentially
    processed_files = 0
    total_nodes_added = 0
    total_edges_added = 0
    for file_path, folder_name, fmt in file_tasks:
        try:
            nodes, edges = process_single_file_ldbcbi(file_path, folder_name, fmt)
            
            # add the nodes to the graph (åŒ…å«æ‰€æœ‰å±æ€§)
            for node_id, node_type, attributes in nodes:
                # æ·»åŠ èŠ‚ç‚¹ï¼ŒåŒ…å«æ‰€æœ‰å±æ€§
                # å°† label ä½œä¸ºå•ç‹¬å±æ€§ï¼ŒåŒæ—¶ä¿ç•™æ‰€æœ‰å…¶ä»–å±æ€§
                node_attrs = {'label': node_type}
                node_attrs.update(attributes)
                graph.add_node(node_id, **node_attrs)
            
            # add the edges to the graph
            for src, dst, rel_type in edges:
                graph.add_edge(src, dst, label=rel_type)
            
            nodes_count = len(nodes)
            edges_count = len(edges)
            total_nodes_added += nodes_count
            total_edges_added += edges_count
            
            processed_files += 1
            
            # print progress with details
            if processed_files % 10 == 0 or processed_files == total_files:
                print(f"progress: {processed_files}/{total_files} files processed (nodes: {total_nodes_added:,}, edges: {total_edges_added:,})")
            elif nodes_count > 0 or edges_count > 0:
                # æ‰“å°æœ‰å†…å®¹çš„æ–‡ä»¶
                print(f"  {os.path.basename(file_path)}: {nodes_count} nodes, {edges_count} edges")
                
        except Exception as e:
            print(f"error when processing the file {os.path.basename(file_path)}: {e}")
            import traceback
            traceback.print_exc()
            processed_files += 1
    
    overall_elapsed = time.time() - overall_start
    print(f"\n{'='*60}")
    print("graph loaded successfully!")
    print(f"{'='*60}")
    print(f"total time: {overall_elapsed:.2f} seconds")
    print(f"processed files: {processed_files}/{total_files}")
    print(f"number of nodes: {graph.number_of_nodes():,}")
    print(f"number of edges: {graph.number_of_edges():,}")
    print(f"{'='*60}\n")
    return graph

def build_graph_from_data_threaded(data_path: str, file_format: str, max_workers: int = 4) -> nx.MultiDiGraph:
    """
    Build the graph from the data path (parallel processing with thread pool)
    Suitable for large dataset scenarios with multiple files
    
    Args:
        data_path: Path to the data directory
        file_format: File format to process (e.g., ".csv.gz", ".csv")
        max_workers: Maximum number of worker threads
        
    Returns:
        nx.MultiDiGraph: The constructed graph
    """
    import time
    overall_start = time.time()
    
    graph = nx.MultiDiGraph()
    print(f"loading graph data from {data_path}...")
    
    # collect all the files to be processed
    file_tasks = []
    for root, dirs, files in os.walk(data_path):
        for file in files:
            if not file.endswith(file_format):
                continue
            file_path = os.path.join(root, file)
            folder_name = os.path.basename(root)
            file_tasks.append((file_path, folder_name, file_format))
    
    total_files = len(file_tasks)
    print(f"found {total_files} files, start to process...")
    
    # use the thread pool to process the files
    processed_files = 0
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {
            executor.submit(process_single_file_ldbcbi, file_path, folder_name, file_format): file_path
            for file_path, folder_name, file_format in file_tasks
        }
        
        # process the completed tasks
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                nodes, edges = future.result()
                
                # add the nodes to the graph (åŒ…å«æ‰€æœ‰å±æ€§)
                for node_id, node_type, attributes in nodes:
                    # æ·»åŠ èŠ‚ç‚¹ï¼ŒåŒ…å«æ‰€æœ‰å±æ€§
                    # å°† label ä½œä¸ºå•ç‹¬å±æ€§ï¼ŒåŒæ—¶ä¿ç•™æ‰€æœ‰å…¶ä»–å±æ€§
                    node_attrs = {'label': node_type}
                    node_attrs.update(attributes)
                    graph.add_node(node_id, **node_attrs)
                
                # add the edges to the graph
                for src, dst, rel_type in edges:
                    graph.add_edge(src, dst, label=rel_type)
                
                processed_files += 1
                    
            except Exception as e:
                print(f"error when processing the file {os.path.basename(file_path)}: {e}")
                processed_files += 1

    overall_elapsed = time.time() - overall_start
    print(f"\n{'='*60}")
    print("graph loaded successfully!")
    print(f"{'='*60}")
    print(f"total time: {overall_elapsed:.2f} seconds")
    print(f"processed files: {processed_files}/{total_files}")
    print(f"number of nodes: {graph.number_of_nodes():,}")
    print(f"number of edges: {graph.number_of_edges():,}")
    print(f"{'='*60}\n")
    return graph

def save_graph(graph: nx.MultiDiGraph, path: str):
    # save the graph to the file
    with open(path, "wb") as f:
        pickle.dump(graph, f)
    print(f"graph saved to {path}")

def load_graph(path: str) -> nx.MultiDiGraph:
    # load the graph from the file
    with open(path, "rb") as f:
        graph = pickle.load(f)
    print(f"graph loaded from {path}")
    return graph

def build_graph_from_kg_csv(csv_path: str, save_path: str = None) -> nx.MultiDiGraph:
    """
    ä» kg.csv æ ¼å¼çš„æ–‡ä»¶æ„å»ºå›¾å¹¶å¯é€‰ä¿å­˜ä¸º .gpickle æ ¼å¼
    
    CSV æ ¼å¼è¦æ±‚åŒ…å«ä»¥ä¸‹åˆ—ï¼š
    - relation: å…³ç³»ç±»å‹
    - x_id, x_type, x_name, x_source: æºèŠ‚ç‚¹ä¿¡æ¯
    - y_id, y_type, y_name, y_source: ç›®æ ‡èŠ‚ç‚¹ä¿¡æ¯
    - version: å¯é€‰ï¼Œç‰ˆæœ¬ä¿¡æ¯
    
    Args:
        csv_path: kg.csv æ–‡ä»¶è·¯å¾„
        save_path: å¯é€‰ï¼Œä¿å­˜å›¾çš„è·¯å¾„ï¼ˆ.gpickle æ ¼å¼ï¼‰
        
    Returns:
        nx.MultiDiGraph: æ„å»ºå¥½çš„å›¾
    """
    import time
    start_time = time.time()
    
    print(f"å¼€å§‹ä» {csv_path} åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®...")
    
    # è¯»å– CSV æ–‡ä»¶
    df = pd.read_csv(csv_path)
    
    # åˆ›å»ºæœ‰å‘å¤šé‡å›¾
    graph = nx.MultiDiGraph()
    
    # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
    for idx, row in df.iterrows():
        # æå–æºèŠ‚ç‚¹ä¿¡æ¯
        x_id = str(row['x_id'])
        x_type = row['x_type']
        x_name = row.get('x_name', '')
        x_source = row.get('x_source', '')
        
        # æå–ç›®æ ‡èŠ‚ç‚¹ä¿¡æ¯
        y_id = str(row['y_id'])
        y_type = row['y_type']
        y_name = row.get('y_name', '')
        y_source = row.get('y_source', '')
        
        # æå–å…³ç³»ä¿¡æ¯
        relation = row['relation']
        display_relation = row.get('display_relation', relation)
        version = row.get('version', None)  # æå–ç‰ˆæœ¬ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        
        # æ„å»ºèŠ‚ç‚¹IDï¼ˆæ ¼å¼ï¼šç±»å‹:IDï¼‰
        x_node_id = f"{x_type}:{x_id}"
        y_node_id = f"{y_type}:{y_id}"
        
        # æ·»åŠ æºèŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not graph.has_node(x_node_id):
            graph.add_node(
                x_node_id,
                label=x_type,
                node_type=x_type,
                node_id=x_id,
                name=x_name,
                source=x_source
            )
        
        # æ·»åŠ ç›®æ ‡èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not graph.has_node(y_node_id):
            graph.add_node(
                y_node_id,
                label=y_type,
                node_type=y_type,
                node_id=y_id,
                name=y_name,
                source=y_source
            )
        
        # æ·»åŠ è¾¹ï¼ˆåŒ…å«ç‰ˆæœ¬ä¿¡æ¯ï¼‰
        edge_attrs = {
            'label': relation,
            'relation': relation,
            'display_relation': display_relation
        }
        if version is not None:
            edge_attrs['version'] = version
        
        graph.add_edge(
            x_node_id,
            y_node_id,
            **edge_attrs
        )
        
        # # è¿›åº¦æ˜¾ç¤º
        # if (idx + 1) % 1000 == 0:
        #     print(f"å·²å¤„ç† {idx + 1}/{len(df)} æ¡è®°å½•...")
    
    elapsed_time = time.time() - start_time
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*60}")
    print("çŸ¥è¯†å›¾è°±åŠ è½½å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"å¤„ç†æ—¶é—´: {elapsed_time:.2f} ç§’")
    print(f"å¤„ç†è®°å½•æ•°: {len(df):,}")
    print(f"èŠ‚ç‚¹æ•°é‡: {graph.number_of_nodes():,}")
    print(f"è¾¹æ•°é‡: {graph.number_of_edges():,}")
    print(f"{'='*60}\n")
    
    # å¦‚æœæŒ‡å®šäº†ä¿å­˜è·¯å¾„ï¼Œåˆ™ä¿å­˜å›¾
    if save_path:
        save_graph(graph, save_path)
    
    return graph


def is_camel_case(filename: str) -> bool:
    """
    åˆ¤æ–­æ–‡ä»¶åæ˜¯å¦ä¸ºé©¼å³°å‘½åæ³•ï¼ˆå…³ç³»æ–‡ä»¶ï¼‰
    å¦‚æœæ–‡ä»¶ååŒ…å«å¤§å†™å­—æ¯ï¼ˆé™¤äº†é¦–å­—æ¯ï¼‰ï¼Œåˆ™è®¤ä¸ºæ˜¯é©¼å³°å‘½å
    """
    # ç§»é™¤æ–‡ä»¶æ‰©å±•å
    name_without_ext = os.path.splitext(filename)[0]
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤§å†™å­—æ¯ï¼ˆé™¤äº†é¦–å­—æ¯ï¼‰
    return any(c.isupper() for c in name_without_ext[1:])


def parse_relation_filename(filename: str) -> Tuple[str, str, str]:
    """
    è§£æå…³ç³»æ–‡ä»¶åï¼Œæå–æºèŠ‚ç‚¹ç±»å‹ã€å…³ç³»åå’Œç›®æ ‡èŠ‚ç‚¹ç±»å‹
    
    ä¾‹å¦‚ï¼š
    - AccountTransferAccount -> (Account, Transfer, Account)
    - PersonInvestCompany -> (Person, Invest, Company)
    - CompanyOwnAccount -> (Company, Own, Account)
    
    Args:
        filename: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        
    Returns:
        (æºèŠ‚ç‚¹ç±»å‹, å…³ç³»å, ç›®æ ‡èŠ‚ç‚¹ç±»å‹)
    """
    name_without_ext = os.path.splitext(filename)[0]
    
    # æ‰¾åˆ°æ‰€æœ‰å¤§å†™å­—æ¯çš„ä½ç½®
    uppercase_positions = [i for i, c in enumerate(name_without_ext) if c.isupper()]
    
    if len(uppercase_positions) < 2:
        # å¦‚æœåªæœ‰ä¸€ä¸ªå¤§å†™å­—æ¯ï¼ˆé¦–å­—æ¯ï¼‰ï¼Œæ— æ³•è§£æ
        return None, None, None
    
    # ç¬¬ä¸€ä¸ªå¤§å†™å­—æ¯ä½ç½®æ˜¯0ï¼ˆé¦–å­—æ¯ï¼‰
    # æ‰¾åˆ°ç¬¬äºŒä¸ªå¤§å†™å­—æ¯çš„ä½ç½®ï¼Œè¿™é€šå¸¸æ˜¯æºèŠ‚ç‚¹ç±»å‹å’Œå…³ç³»åçš„åˆ†ç•Œ
    first_break = uppercase_positions[1] if len(uppercase_positions) > 1 else len(name_without_ext)
    
    # æºèŠ‚ç‚¹ç±»å‹ï¼šä»å¼€å¤´åˆ°ç¬¬ä¸€ä¸ªåˆ†ç•Œç‚¹
    src_type = name_without_ext[:first_break]
    
    # æ‰¾åˆ°æœ€åä¸€ä¸ªå¤§å†™å­—æ¯çš„ä½ç½®ï¼Œè¿™é€šå¸¸æ˜¯å…³ç³»åå’Œç›®æ ‡èŠ‚ç‚¹ç±»å‹çš„åˆ†ç•Œ
    if len(uppercase_positions) >= 3:
        # æœ‰å¤šä¸ªå¤§å†™å­—æ¯ï¼Œæœ€åä¸€ä¸ªåˆ†ç•Œç‚¹æ˜¯å€’æ•°ç¬¬äºŒä¸ªå¤§å†™å­—æ¯
        last_break = uppercase_positions[-1]
        # å…³ç³»åï¼šä»ç¬¬ä¸€ä¸ªåˆ†ç•Œç‚¹åˆ°æœ€åä¸€ä¸ªåˆ†ç•Œç‚¹
        rel_name = name_without_ext[first_break:last_break]
        # ç›®æ ‡èŠ‚ç‚¹ç±»å‹ï¼šä»æœ€åä¸€ä¸ªåˆ†ç•Œç‚¹åˆ°ç»“å°¾
        dst_type = name_without_ext[last_break:]
    else:
        # åªæœ‰ä¸¤ä¸ªå¤§å†™å­—æ¯ï¼Œè¯´æ˜æ˜¯ SourceTarget æ ¼å¼
        # è¿™ç§æƒ…å†µä¸‹ï¼Œä¸­é—´éƒ¨åˆ†å¯èƒ½æ˜¯å…³ç³»åï¼Œä½†é€šå¸¸å…³ç³»åä¼šè¢«çœç•¥
        # ä¾‹å¦‚ï¼šAccountAccount å¯èƒ½æ˜¯ Account -> Account çš„è‡ªç¯å…³ç³»
        # æˆ‘ä»¬å‡è®¾ç¬¬äºŒä¸ªå¤§å†™å­—æ¯å¼€å§‹æ˜¯ç›®æ ‡èŠ‚ç‚¹ç±»å‹
        last_break = uppercase_positions[1]
        rel_name = name_without_ext[first_break:last_break] if first_break < last_break else name_without_ext[first_break:]
        dst_type = name_without_ext[last_break:] if last_break < len(name_without_ext) else src_type
    
    return src_type, rel_name, dst_type


def process_node_file_ldbcfin(file_path: str) -> List[Tuple[str, str, dict]]:
    """
    å¤„ç†èŠ‚ç‚¹æ–‡ä»¶ï¼Œè¿”å›èŠ‚ç‚¹åˆ—è¡¨ï¼ˆåŒ…å«æ‰€æœ‰å±æ€§ï¼‰
    
    Args:
        file_path: CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(node_id, node_type, attributes_dict), ...]
        å…¶ä¸­ attributes_dict åŒ…å«èŠ‚ç‚¹çš„æ‰€æœ‰å±æ€§ï¼ˆåŒ…æ‹¬IDåˆ—ï¼‰
    """
    nodes = []
    try:
        # è¯»å–CSVæ–‡ä»¶ï¼Œä½¿ç”¨ | åˆ†éš”ç¬¦
        # æ³¨æ„ï¼šLDBC FinBench çš„ CSV æ–‡ä»¶ç¬¬ä¸€è¡Œæ˜¯åˆ—åï¼Œæ‰€ä»¥ä½¿ç”¨ header=0ï¼ˆé»˜è®¤å€¼ï¼‰
        df = read_csv(file_path, sep='|', header=0)
        
        if df.empty:
            return nodes
        
        # èŠ‚ç‚¹ç±»å‹æ˜¯æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        node_type = os.path.splitext(os.path.basename(file_path))[0]
        
        # æŸ¥æ‰¾IDåˆ—ï¼šé€šå¸¸æ˜¯ç¬¬ä¸€åˆ—ï¼Œæˆ–è€…åŒ…å«'id'çš„åˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        id_col = None
        for col in df.columns:
            if 'id' in str(col).lower():
                id_col = col
                break
        
        if id_col is None and len(df.columns) > 0:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°idåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—
            id_col = df.columns[0]
        
        if id_col is not None:
            # éå†æ¯ä¸€è¡Œï¼Œæ„å»ºèŠ‚ç‚¹åŠå…¶å±æ€§
            for idx, row in df.iterrows():
                node_id_value = str(row[id_col])
                node_id = f"{node_type}:{node_id_value}"
                
                # æ„å»ºå±æ€§å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰åˆ—çš„å€¼
                attributes = {}
                for col in df.columns:
                    value = row[col]
                    # å¤„ç† NaN å€¼
                    if pd.isna(value):
                        attributes[col] = None
                    else:
                        # ä¿æŒåŸå§‹ç±»å‹ï¼Œä½†ç¡®ä¿å¯ä»¥åºåˆ—åŒ–
                        attributes[col] = value
                
                nodes.append((node_id, node_type, attributes))
            
    except Exception as e:
        print(f"å¤„ç†èŠ‚ç‚¹æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    return nodes


def process_relation_file_ldbcfin(file_path: str) -> List[Tuple[str, str, str]]:
    """
    å¤„ç†å…³ç³»æ–‡ä»¶ï¼Œè¿”å›è¾¹åˆ—è¡¨
    
    Args:
        file_path: CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        è¾¹åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(src_id, dst_id, rel_type), ...]
    """
    edges = []
    try:
        # è¯»å–CSVæ–‡ä»¶ï¼Œä½¿ç”¨ | åˆ†éš”ç¬¦
        df = read_csv(file_path, sep='|')
        
        if df.empty:
            return edges
        
        # è§£ææ–‡ä»¶åè·å–å…³ç³»ä¿¡æ¯
        filename = os.path.basename(file_path)
        src_type, rel_name, dst_type = parse_relation_filename(filename)
        
        if src_type is None or dst_type is None:
            print(f"è­¦å‘Š: æ— æ³•è§£æå…³ç³»æ–‡ä»¶å {filename}ï¼Œè·³è¿‡")
            return edges
        
        # å…³ç³»ç±»å‹ä½¿ç”¨å®Œæ•´çš„å…³ç³»å
        rel_type = f"{src_type}_{rel_name}_{dst_type}" if rel_name else f"{src_type}_to_{dst_type}"
        
        # æŸ¥æ‰¾æºèŠ‚ç‚¹IDåˆ—å’Œç›®æ ‡èŠ‚ç‚¹IDåˆ—
        # å¸¸è§çš„åˆ—åæ¨¡å¼ï¼š
        # - fromId, toId
        # - srcId, dstId
        # - sourceId, targetId
        # - æˆ–è€…ç‰¹å®šç±»å‹ï¼šå¦‚ investorId, companyId
        
        src_col = None
        dst_col = None
        
        # å…ˆå°è¯•å¸¸è§çš„åˆ—å
        for col in df.columns:
            col_lower = str(col).lower()
            if 'from' in col_lower or 'src' in col_lower or 'source' in col_lower:
                src_col = col
            elif 'to' in col_lower or 'dst' in col_lower or 'target' in col_lower:
                dst_col = col
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ ¹æ®èŠ‚ç‚¹ç±»å‹æŸ¥æ‰¾
        if src_col is None:
            for col in df.columns:
                if src_type.lower() in str(col).lower() and 'id' in str(col).lower():
                    src_col = col
                    break
        
        if dst_col is None:
            for col in df.columns:
                if dst_type.lower() in str(col).lower() and 'id' in str(col).lower():
                    dst_col = col
                    break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å‰ä¸¤åˆ—
        if src_col is None and len(df.columns) >= 1:
            src_col = df.columns[0]
        if dst_col is None and len(df.columns) >= 2:
            dst_col = df.columns[1]
        
        if src_col is None or dst_col is None:
            print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°æºèŠ‚ç‚¹æˆ–ç›®æ ‡èŠ‚ç‚¹IDåˆ—ï¼Œæ–‡ä»¶ {filename}ï¼Œåˆ—: {list(df.columns)}")
            return edges
        
        # æ„å»ºè¾¹
        src_values = df[src_col].astype(str).values
        dst_values = df[dst_col].astype(str).values
        
        edges = [
            (f"{src_type}:{src}", f"{dst_type}:{dst}", rel_type)
            for src, dst in zip(src_values, dst_values)
        ]
        
    except Exception as e:
        print(f"å¤„ç†å…³ç³»æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    return edges


def build_graph_from_data_ldbcfin(data_path: str, file_format: str = ".csv") -> nx.MultiDiGraph:
    """
    ä» LDBC SNB FinBench æ•°æ®æ„å»ºå›¾
    
    æ•°æ®æ ¼å¼è¯´æ˜ï¼š
    - é©¼å³°å‘½åæ³•çš„æ–‡ä»¶ï¼ˆå¦‚ AccountTransferAccount.csvï¼‰æ˜¯å…³ç³»æ–‡ä»¶
    - å…¶ä»–æ–‡ä»¶ï¼ˆå¦‚ Account.csvï¼‰æ˜¯èŠ‚ç‚¹æ–‡ä»¶
    - æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹
    
    Args:
        data_path: æ•°æ®ç›®å½•è·¯å¾„
        file_format: æ–‡ä»¶æ ¼å¼ï¼ˆé»˜è®¤ ".csv"ï¼‰
        
    Returns:
        nx.MultiDiGraph: æ„å»ºå¥½çš„å›¾
    """
    import time
    overall_start = time.time()
    
    graph = nx.MultiDiGraph()
    print(f"ä» {data_path} åŠ è½½å›¾æ•°æ®...")
    
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
    all_files = []
    if os.path.isdir(data_path):
        for file in os.listdir(data_path):
            if file.endswith(file_format):
                file_path = os.path.join(data_path, file)
                all_files.append(file_path)
    else:
        print(f"é”™è¯¯: {data_path} ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç›®å½•")
        return graph
    
    total_files = len(all_files)
    print(f"æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    
    # å…ˆå¤„ç†èŠ‚ç‚¹æ–‡ä»¶ï¼Œå†å¤„ç†å…³ç³»æ–‡ä»¶
    node_files = []
    relation_files = []
    
    for file_path in all_files:
        filename = os.path.basename(file_path)
        if is_camel_case(filename):
            relation_files.append(file_path)
        else:
            node_files.append(file_path)
    
    print(f"èŠ‚ç‚¹æ–‡ä»¶: {len(node_files)} ä¸ªï¼Œå…³ç³»æ–‡ä»¶: {len(relation_files)} ä¸ª")
    
    # å¤„ç†èŠ‚ç‚¹æ–‡ä»¶
    total_nodes_added = 0
    processed_files = 0
    
    for file_path in node_files:
        try:
            nodes = process_node_file_ldbcfin(file_path)
            for node_id, node_type, attributes in nodes:
                # æ·»åŠ èŠ‚ç‚¹ï¼ŒåŒ…å«æ‰€æœ‰å±æ€§
                # å°† label ä½œä¸ºå•ç‹¬å±æ€§ï¼ŒåŒæ—¶ä¿ç•™æ‰€æœ‰å…¶ä»–å±æ€§
                node_attrs = {'label': node_type}
                node_attrs.update(attributes)
                graph.add_node(node_id, **node_attrs)
            
            nodes_count = len(nodes)
            total_nodes_added += nodes_count
            processed_files += 1
            
            if nodes_count > 0:
                print(f"  {os.path.basename(file_path)}: {nodes_count} ä¸ªèŠ‚ç‚¹")
                
        except Exception as e:
            print(f"å¤„ç†èŠ‚ç‚¹æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºé”™: {e}")
            processed_files += 1
    
    # å¤„ç†å…³ç³»æ–‡ä»¶
    total_edges_added = 0
    
    for file_path in relation_files:
        try:
            edges = process_relation_file_ldbcfin(file_path)
            for src, dst, rel_type in edges:
                graph.add_edge(src, dst, label=rel_type)
            
            edges_count = len(edges)
            total_edges_added += edges_count
            processed_files += 1
            
            if edges_count > 0:
                print(f"  {os.path.basename(file_path)}: {edges_count} æ¡è¾¹")
                
        except Exception as e:
            print(f"å¤„ç†å…³ç³»æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            processed_files += 1
    
    overall_elapsed = time.time() - overall_start
    
    print(f"\n{'='*60}")
    print("å›¾åŠ è½½æˆåŠŸï¼")
    print(f"{'='*60}")
    print(f"æ€»è€—æ—¶: {overall_elapsed:.2f} ç§’")
    print(f"å¤„ç†æ–‡ä»¶æ•°: {processed_files}/{total_files}")
    print(f"èŠ‚ç‚¹æ•°é‡: {graph.number_of_nodes():,}")
    print(f"è¾¹æ•°é‡: {graph.number_of_edges():,}")
    print(f"{'='*60}\n")
    
    return graph


