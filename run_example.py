#!/usr/bin/env python3
"""
NGDB benchmarkè¿è¡Œç¤ºä¾‹è„šæœ¬
"""

import sys
import os
import yaml
from ngdb_framework import NGDBBench
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ¬¢è¿ä½¿ç”¨NGDB benchmarkæ¡†æ¶!")
    print("=" * 50)
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–é…ç½®
    config_path = os.path.join(os.path.dirname(__file__), 'configs', 'default_config.yaml')
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    print(f"ğŸ“„ å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    
    try:
        print("ğŸ“Š åˆ›å»ºNGDBæ¡†æ¶å®ä¾‹...")
        framework = NGDBBench(config)
        
        print("ğŸ”„ å¼€å§‹è¿è¡ŒåŸºå‡†æµ‹è¯•...")
        results = framework.run_benchmark()
        
        print("\n" + "=" * 50)
        print("ğŸ“ˆ åŸºå‡†æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 50)
        
        # æ‰§è¡Œä¿¡æ¯
        exec_metadata = results.get('execution_metadata', {})
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {exec_metadata.get('total_time', 0):.2f}ç§’")
        
        # æ•°æ®ä¿¡æ¯
        data_info = results.get('data_info', {})
        original_stats = data_info.get('original_graph_stats', {})
        perturbed_stats = data_info.get('perturbed_graph_stats', {})
        
        print(f"ğŸ“Š åŸå§‹å›¾: {original_stats.get('num_nodes', 0)}ä¸ªèŠ‚ç‚¹, {original_stats.get('num_edges', 0)}æ¡è¾¹")
        print(f"ğŸ”€ æ‰°åŠ¨å›¾: {perturbed_stats.get('num_nodes', 0)}ä¸ªèŠ‚ç‚¹, {perturbed_stats.get('num_edges', 0)}æ¡è¾¹")
        
        # æ‰°åŠ¨ä¿¡æ¯
        perturbation_info = data_info.get('perturbation_info', {})
        if 'operations' in perturbation_info:
            print(f"ğŸ¯ æ‰°åŠ¨æ“ä½œ: {len(perturbation_info['operations'])}ä¸ª")
        
        # æŸ¥è¯¢ä¿¡æ¯
        queries_info = results.get('queries_info', {})
        print(f"â“ æŸ¥è¯¢æ•°é‡: {queries_info.get('total_queries', 0)}")
        
        # è¯„ä¼°ç»“æœ
        evaluation_results = results.get('evaluation_results', {})
        
        print("\nğŸ“‹ è¯„ä¼°ç»“æœ:")
        
        if 'accuracy_evaluation' in evaluation_results:
            acc_metrics = evaluation_results['accuracy_evaluation'].get('metrics', {})
            accuracy = acc_metrics.get('accuracy', 0)
            match_rate = acc_metrics.get('match_rate', 0)
            print(f"  âœ… å‡†ç¡®æ€§: {accuracy:.3f}")
            print(f"  ğŸ¯ åŒ¹é…ç‡: {match_rate:.3f}")
        
        if 'robustness_evaluation' in evaluation_results:
            rob_metrics = evaluation_results['robustness_evaluation'].get('robustness_metrics', {})
            overall_robustness = rob_metrics.get('overall_robustness', 0)
            print(f"  ğŸ›¡ï¸  é²æ£’æ€§: {overall_robustness:.3f}")
        
        if 'performance_evaluation' in evaluation_results:
            perf_metrics = evaluation_results['performance_evaluation'].get('execution_metrics', {})
            avg_query_time = perf_metrics.get('average_query_time', 0)
            print(f"  âš¡ å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_query_time:.4f}ç§’")
        
        # ç»¼åˆæŠ¥å‘Š
        reports = results.get('reports', {})
        if 'comprehensive_report' in reports:
            comp_report = reports['comprehensive_report']
            exec_summary = comp_report.get('executive_summary', {})
            overall_perf = exec_summary.get('overall_performance', {})
            
            score = overall_perf.get('score', 0)
            grade = overall_perf.get('grade', 'N/A')
            print(f"\nğŸ† æ€»ä½“æ€§èƒ½è¯„åˆ†: {score:.3f} (ç­‰çº§: {grade})")
            
            # å…³é”®å‘ç°
            key_findings = exec_summary.get('key_findings', [])
            if key_findings:
                print("\nğŸ” å…³é”®å‘ç°:")
                for finding in key_findings[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                    print(f"  â€¢ {finding}")
            
            # ä¸»è¦å…³æ³¨ç‚¹
            main_concerns = exec_summary.get('main_concerns', [])
            if main_concerns:
                print("\nâš ï¸  ä¸»è¦å…³æ³¨ç‚¹:")
                for concern in main_concerns[:2]:  # æ˜¾ç¤ºå‰2ä¸ª
                    print(f"  â€¢ {concern.get('description', 'N/A')}")
        
        print("\n" + "=" * 50)
        print("âœ… åŸºå‡†æµ‹è¯•å®Œæˆ!")
        
        # ä¿å­˜ç»“æœ
        framework.save_results("ngdb_example_results.json")
        print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: ngdb_example_results.json")
        
        # æ€§èƒ½è¯„çº§
        if 'comprehensive_report' in reports:
            score = reports['comprehensive_report'].get('executive_summary', {}).get('overall_performance', {}).get('score', 0)
            if score >= 0.8:
                print("ğŸŒŸ ç®—æ³•æ€§èƒ½ä¼˜ç§€!")
            elif score >= 0.6:
                print("ğŸ‘ ç®—æ³•æ€§èƒ½è‰¯å¥½!")
            elif score >= 0.4:
                print("âš¡ ç®—æ³•æ€§èƒ½ä¸€èˆ¬ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
            else:
                print("ğŸ”§ ç®—æ³•æ€§èƒ½éœ€è¦æ˜¾è‘—æ”¹è¿›")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
