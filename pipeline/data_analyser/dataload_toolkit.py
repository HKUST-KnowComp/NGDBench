""" 
temporarily done for only one data format, need to be extended to other data formats
"""
import os
import gzip
import pandas as pd
import networkx as nx
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Tuple, List
from graph_handler import GraphInspector
from pathlib import Path
import pickle
import random

def read_csv_gz(file_path: str) -> pd.DataFrame:
    # read the single .csv.gz file
    with gzip.open(file_path, 'rt', encoding='utf-8') as f:
        return pd.read_csv(f)

def process_single_file(file_path: str, folder_name: str, file_format: str) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str, str]]]:
    # Process the single file and return the nodes and edges
    import time
    start_time = time.time()
    file_name = os.path.basename(file_path)
    # print(f"ğŸ“– å¼€å§‹å¤„ç†: {file_name}")
    
    nodes = []
    edges = []
    
    try:
        if file_format == ".csv.gz":
            df = read_csv_gz(file_path)
        # elif file_format == ".json":
        #     df = read_json(file_path)
        # elif file_format == ".jsonl":
        #     df = read_jsonl(file_path)
        # elif file_format == ".parquet":
        #     df = read_parquet(file_path)
        # elif file_format == ".feather":
        #     df = read_feather(file_path)
    except Exception as e:
        print(f"skip the file {file_path}, error: {e}")
        return nodes, edges
    
    # check if the file is a node table or an edge table
    if "_" not in folder_name:
        # the file is a node table
        node_type = folder_name
        if 'id' in df.columns:
            # for nid in df['id'].astype(str):
            #     nodes.append((f"{node_type}:{nid}", node_type))
            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼Œæ¯” iterrows å¿«å¾—å¤š
            node_ids = df['id'].astype(str).values
            nodes = [(f"{node_type}:{nid}", node_type) for nid in node_ids]
    else:
        # the file is an edge table
        rel_type = folder_name
        cols = df.columns.tolist()
        if len(cols) >= 2:
            src_col, dst_col = cols[0], cols[1]
            src_prefix = src_col.split('_')[0]
            dst_prefix = dst_col.split('_')[0]
            
            edges = []

            src_values = df[src_col].values
            dst_values = df[dst_col].values
            edges = [
                (f"{src_prefix}:{src}", f"{dst_prefix}:{dst}", rel_type)
                for src, dst in zip(src_values, dst_values)
            ]
    
    return nodes, edges

def build_graph_from_data(data_path: str, file_format: str) -> nx.MultiDiGraph:
    """
    Build the graph from the data path (sequential processing)
    Suitable for single file or small dataset scenarios
    
    Args:
        data_path: Path to the data directory
        file_format: File format to process (e.g., ".csv.gz", ".csv")
        
    Returns:
        nx.MultiDiGraph: The constructed graph
    """
    import time
    overall_start = time.time()
    
    graph = nx.MultiDiGraph()
    print(f"loading graph data from {data_path}...")
    
    # collect all the files to be processed
    file_tasks = []
    for root, dirs, files in os.walk(data_path):
        for file in files:
            if not file.endswith(file_format):
                continue
            file_path = os.path.join(root, file)
            folder_name = os.path.basename(root)
            file_tasks.append((file_path, folder_name, file_format))
    
    total_files = len(file_tasks)
    print(f"found {total_files} files, start to process...")
    
    # process files sequentially
    processed_files = 0
    for file_path, folder_name, fmt in file_tasks:
        try:
            nodes, edges = process_single_file(file_path, folder_name, fmt)
            
            # add the nodes to the graph
            for node_id, node_type in nodes:
                graph.add_node(node_id, label=node_type)
            
            # add the edges to the graph
            for src, dst, rel_type in edges:
                graph.add_edge(src, dst, label=rel_type)
            
            processed_files += 1
            
            # print progress
            if processed_files % 10 == 0 or processed_files == total_files:
                print(f"progress: {processed_files}/{total_files} files processed")
                
        except Exception as e:
            print(f"error when processing the file {os.path.basename(file_path)}: {e}")
            processed_files += 1
    
    overall_elapsed = time.time() - overall_start
    print(f"\n{'='*60}")
    print("graph loaded successfully!")
    print(f"{'='*60}")
    print(f"total time: {overall_elapsed:.2f} seconds")
    print(f"processed files: {processed_files}/{total_files}")
    print(f"number of nodes: {graph.number_of_nodes():,}")
    print(f"number of edges: {graph.number_of_edges():,}")
    print(f"{'='*60}\n")
    return graph

def build_graph_from_data_threaded(data_path: str, file_format: str, max_workers: int = 4) -> nx.MultiDiGraph:
    """
    Build the graph from the data path (parallel processing with thread pool)
    Suitable for large dataset scenarios with multiple files
    
    Args:
        data_path: Path to the data directory
        file_format: File format to process (e.g., ".csv.gz", ".csv")
        max_workers: Maximum number of worker threads
        
    Returns:
        nx.MultiDiGraph: The constructed graph
    """
    import time
    overall_start = time.time()
    
    graph = nx.MultiDiGraph()
    print(f"loading graph data from {data_path}...")
    
    # collect all the files to be processed
    file_tasks = []
    for root, dirs, files in os.walk(data_path):
        for file in files:
            if not file.endswith(file_format):
                continue
            file_path = os.path.join(root, file)
            folder_name = os.path.basename(root)
            file_tasks.append((file_path, folder_name, file_format))
    
    total_files = len(file_tasks)
    print(f"found {total_files} files, start to process...")
    
    # use the thread pool to process the files
    processed_files = 0
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {
            executor.submit(process_single_file, file_path, folder_name, file_format): file_path
            for file_path, folder_name, file_format in file_tasks
        }
        
        # process the completed tasks
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                nodes, edges = future.result()
                
                # add the nodes to the graph
                for node_id, node_type in nodes:
                    graph.add_node(node_id, label=node_type)
                
                # add the edges to the graph
                for src, dst, rel_type in edges:
                    graph.add_edge(src, dst, label=rel_type)
                
                processed_files += 1
                    
            except Exception as e:
                print(f"error when processing the file {os.path.basename(file_path)}: {e}")
                processed_files += 1

    overall_elapsed = time.time() - overall_start
    print(f"\n{'='*60}")
    print("graph loaded successfully!")
    print(f"{'='*60}")
    print(f"total time: {overall_elapsed:.2f} seconds")
    print(f"processed files: {processed_files}/{total_files}")
    print(f"number of nodes: {graph.number_of_nodes():,}")
    print(f"number of edges: {graph.number_of_edges():,}")
    print(f"{'='*60}\n")
    return graph

def save_graph(graph: nx.MultiDiGraph, path: str):
    # save the graph to the file
    with open(path, "wb") as f:
        pickle.dump(graph, f)
    print(f"graph saved to {path}")

def load_graph(path: str) -> nx.MultiDiGraph:
    # load the graph from the file
    with open(path, "rb") as f:
        graph = pickle.load(f)
    print(f"graph loaded from {path}")
    return graph

def build_graph_from_kg_csv(csv_path: str, save_path: str = None) -> nx.MultiDiGraph:
    """
    ä» kg.csv æ ¼å¼çš„æ–‡ä»¶æ„å»ºå›¾å¹¶å¯é€‰ä¿å­˜ä¸º .gpickle æ ¼å¼
    
    CSV æ ¼å¼è¦æ±‚åŒ…å«ä»¥ä¸‹åˆ—ï¼š
    - relation: å…³ç³»ç±»å‹
    - x_id, x_type, x_name, x_source: æºèŠ‚ç‚¹ä¿¡æ¯
    - y_id, y_type, y_name, y_source: ç›®æ ‡èŠ‚ç‚¹ä¿¡æ¯
    - version: å¯é€‰ï¼Œç‰ˆæœ¬ä¿¡æ¯
    
    Args:
        csv_path: kg.csv æ–‡ä»¶è·¯å¾„
        save_path: å¯é€‰ï¼Œä¿å­˜å›¾çš„è·¯å¾„ï¼ˆ.gpickle æ ¼å¼ï¼‰
        
    Returns:
        nx.MultiDiGraph: æ„å»ºå¥½çš„å›¾
    """
    import time
    start_time = time.time()
    
    print(f"å¼€å§‹ä» {csv_path} åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®...")
    
    # è¯»å– CSV æ–‡ä»¶
    df = pd.read_csv(csv_path)
    
    # åˆ›å»ºæœ‰å‘å¤šé‡å›¾
    graph = nx.MultiDiGraph()
    
    # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
    for idx, row in df.iterrows():
        # æå–æºèŠ‚ç‚¹ä¿¡æ¯
        x_id = str(row['x_id'])
        x_type = row['x_type']
        x_name = row.get('x_name', '')
        x_source = row.get('x_source', '')
        
        # æå–ç›®æ ‡èŠ‚ç‚¹ä¿¡æ¯
        y_id = str(row['y_id'])
        y_type = row['y_type']
        y_name = row.get('y_name', '')
        y_source = row.get('y_source', '')
        
        # æå–å…³ç³»ä¿¡æ¯
        relation = row['relation']
        display_relation = row.get('display_relation', relation)
        version = row.get('version', None)  # æå–ç‰ˆæœ¬ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        
        # æ„å»ºèŠ‚ç‚¹IDï¼ˆæ ¼å¼ï¼šç±»å‹:IDï¼‰
        x_node_id = f"{x_type}:{x_id}"
        y_node_id = f"{y_type}:{y_id}"
        
        # æ·»åŠ æºèŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not graph.has_node(x_node_id):
            graph.add_node(
                x_node_id,
                label=x_type,
                node_type=x_type,
                node_id=x_id,
                name=x_name,
                source=x_source
            )
        
        # æ·»åŠ ç›®æ ‡èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not graph.has_node(y_node_id):
            graph.add_node(
                y_node_id,
                label=y_type,
                node_type=y_type,
                node_id=y_id,
                name=y_name,
                source=y_source
            )
        
        # æ·»åŠ è¾¹ï¼ˆåŒ…å«ç‰ˆæœ¬ä¿¡æ¯ï¼‰
        edge_attrs = {
            'label': relation,
            'relation': relation,
            'display_relation': display_relation
        }
        if version is not None:
            edge_attrs['version'] = version
        
        graph.add_edge(
            x_node_id,
            y_node_id,
            **edge_attrs
        )
        
        # # è¿›åº¦æ˜¾ç¤º
        # if (idx + 1) % 1000 == 0:
        #     print(f"å·²å¤„ç† {idx + 1}/{len(df)} æ¡è®°å½•...")
    
    elapsed_time = time.time() - start_time
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*60}")
    print("çŸ¥è¯†å›¾è°±åŠ è½½å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"å¤„ç†æ—¶é—´: {elapsed_time:.2f} ç§’")
    print(f"å¤„ç†è®°å½•æ•°: {len(df):,}")
    print(f"èŠ‚ç‚¹æ•°é‡: {graph.number_of_nodes():,}")
    print(f"è¾¹æ•°é‡: {graph.number_of_edges():,}")
    print(f"{'='*60}\n")
    
    # å¦‚æœæŒ‡å®šäº†ä¿å­˜è·¯å¾„ï¼Œåˆ™ä¿å­˜å›¾
    if save_path:
        save_graph(graph, save_path)
    
    return graph

if __name__ == "__main__":
    data_path = "/home/ylivm/ngdb/ngdb_benchmark/data_gen/perturbed_dataset/ldbc_snb_bi_2510280002/out-sf1/graphs/csv/bi/composite-projected-fk/initial_snapshot"
    graph_name = "ldbc_snb_bi_2510280002"
    file_format = ".csv.gz"
    graph_path = Path(f"pipeline/data_analyser/buffer/{graph_name}.gpickle")
    if graph_path.exists():
        graph = load_graph(graph_path)
        print(f"loaded graph from {graph_path}")
    else:
        graph = build_graph_from_data_threaded(data_path, file_format)
        save_graph(graph, graph_path)
    
    # åˆ›å»ºå›¾æ£€æŸ¥å™¨
    graph_inspector = GraphInspector(graph)
    
    # æ˜¾ç¤ºå›¾çš„ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*60)
    print("ã€å›¾çš„æ•´ä½“ç»Ÿè®¡ä¿¡æ¯ã€‘")
    print("="*60)
    graph_inspector.summary()
    
    # éšæœºé‡‡æ ·ä¸€äº›èŠ‚ç‚¹è¿›è¡Œæµ‹è¯•
    all_nodes = list(graph.nodes())
    sample_size = min(5, len(all_nodes))  # é‡‡æ ·5ä¸ªèŠ‚ç‚¹ï¼Œå¦‚æœèŠ‚ç‚¹æ•°å°‘äº5åˆ™å…¨éƒ¨é‡‡æ ·
    sampled_nodes = random.sample(all_nodes, sample_size)
    
    print("\n" + "="*60)
    print(f"ã€éšæœºé‡‡æ · {sample_size} ä¸ªèŠ‚ç‚¹è¿›è¡Œæµ‹è¯•ã€‘")
    print("="*60)
    
    for i, node in enumerate(sampled_nodes, 1):
        print(f"\n{'â”€'*60}")
        print(f"ğŸ“ èŠ‚ç‚¹ {i}: {node}")
        print(f"{'â”€'*60}")
        
        # æµ‹è¯•åº¦æ•°ç›¸å…³åŠŸèƒ½
        in_deg = graph_inspector.in_degree(node)
        out_deg = graph_inspector.out_degree(node)
        total_deg = graph_inspector.degree(node)
        print(f"ğŸ“¥ å…¥åº¦: {in_deg}")
        print(f"ğŸ“¤ å‡ºåº¦: {out_deg}")
        print(f"ğŸ“Š æ€»åº¦æ•°: {total_deg}")
        
        # æµ‹è¯•æŒ‰å…³ç³»ç»Ÿè®¡å‡ºåº¦
        rel_outdegree = graph_inspector.out_degree_by_relation(node)
        if rel_outdegree:
            print(f"\nğŸ”— æŒ‰å…³ç³»ç±»å‹ç»Ÿè®¡å‡ºåº¦:")                                                                                                                                          
            for rel, count in sorted(rel_outdegree.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {rel}: {count}")
        else:
            print(f"\nğŸ”— è¯¥èŠ‚ç‚¹æ²¡æœ‰å‡ºè¾¹")
        
        # æµ‹è¯•å…¥è¾¹å’Œå‡ºè¾¹
        in_edges = graph_inspector.in_edges(node)
        out_edges = graph_inspector.out_edges(node)
        
        # æ˜¾ç¤ºéƒ¨åˆ†å…¥è¾¹ç¤ºä¾‹ï¼ˆæœ€å¤šæ˜¾ç¤º3æ¡ï¼‰
        if in_edges:
            print(f"\nğŸ“¥ å…¥è¾¹ç¤ºä¾‹ (å…± {len(in_edges)} æ¡ï¼Œæ˜¾ç¤ºå‰3æ¡):")
            for src, dst, data in in_edges[:3]:
                print(f"  {src} --[{data.get('label', 'N/A')}]--> {dst}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†å‡ºè¾¹ç¤ºä¾‹ï¼ˆæœ€å¤šæ˜¾ç¤º3æ¡ï¼‰
        if out_edges:
            print(f"\nğŸ“¤ å‡ºè¾¹ç¤ºä¾‹ (å…± {len(out_edges)} æ¡ï¼Œæ˜¾ç¤ºå‰3æ¡):")
            for src, dst, data in out_edges[:3]:
                print(f"  {src} --[{data.get('label', 'N/A')}]--> {dst}")
        
        # å¦‚æœæœ‰å…³ç³»ç±»å‹ï¼Œæµ‹è¯•æŒ‰å…³ç³»æŸ¥è¯¢è¾¹
        if rel_outdegree:
            # é€‰æ‹©å‡ºåº¦æœ€é«˜çš„å…³ç³»ç±»å‹
            top_relation = max(rel_outdegree.items(), key=lambda x: x[1])[0]
            edges_of_relation = graph_inspector.edges_by_relation(node, top_relation)
            print(f"\nğŸ¯ å…³ç³»ç±»å‹ '{top_relation}' çš„è¾¹ (å…± {len(edges_of_relation)} æ¡ï¼Œæ˜¾ç¤ºå‰3æ¡):")
            for src, dst in edges_of_relation[:3]:
                print(f"  {src} --> {dst}")
    
    print("\n" + "="*60)
    print("âœ… GraphInspector åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
    print("="*60)