git clone https://github.com/ldbc/ldbc_snb_datagen_spark.git
cd ldbc_snb_datagen_spark
sbt assembly
uv init --python 3.8.20
uv add ./tools
uv sync
scripts/get-spark-to-home.sh
export SPARK_HOME="${HOME}/spark-3.2.2-bin-hadoop3.2"
export PATH="${SPARK_HOME}/bin":"${PATH}"
scripts/build.sh
export PLATFORM_VERSION=$(sbt -batch -error 'print platformVersion')
export DATAGEN_VERSION=$(sbt -batch -error 'print version')
export LDBC_SNB_DATAGEN_JAR=$(sbt -batch -error 'print assembly / assemblyOutputPath')
./tools/run.py <runtime configuration arguments> -- <generator configuration arguments>
export SF=${desired_scale_factor}
export LDBC_SNB_DATAGEN_MAX_MEM=${available_memory}
export LDBC_SNB_DATAGEN_JAR=$(sbt -batch -error 'print assembly / assemblyOutputPath')

rm -rf out-sf${SF}/
tools/run.py \
    --cores $(nproc) \
    --memory ${LDBC_SNB_DATAGEN_MAX_MEM} \
    -- \
    --format csv \
    --scale-factor ${SF} \
    --explode-edges \
    --mode bi \
    --output-dir out-sf${SF}/ \
    --format-options header=false,quoteAll=true,compression=gzip

git clone https://github.com/ldbc/ldbc_snb_bi.git
cd neo4j
export SF=1
export LDBC_SNB_DATAGEN_DIR=${/home/ylivm/ngdb}/ldbc_snb_datagen_spark
. scripts/use-datagen-data-set.sh
export NEO4J_ENV_VARS="${NEO4J_ENV_VARS-} --env NEO4J_dbms_memory_pagecache_size=20G --env NEO4J_dbms_memory_heap_max__size=20G"
scripts/load-in-one-step.sh

docker exec -it snb-bi-neo4j cypher-shell